{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e7c8b8-3c81-4b8e-821e-aa958c360453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class Anchors(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
    "        super(Anchors, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        # compute anchors over all pyramid levels\n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32))\n",
    "        \n",
    "\n",
    "def generate_anchors(base_size=16, ratios=None, scales=None):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales w.r.t. a reference window.\n",
    "    \"\"\"\n",
    "\n",
    "    if ratios is None:\n",
    "        ratios = np.array([0.5, 1, 2])\n",
    "\n",
    "    if scales is None:\n",
    "        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    num_anchors = len(ratios) * len(scales)\n",
    "\n",
    "    # initialize output anchors\n",
    "    anchors = np.zeros((num_anchors, 4))\n",
    "\n",
    "    # scale base_size\n",
    "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "    # compute areas of anchors\n",
    "    areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "    # correct for ratios\n",
    "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "    return anchors\n",
    "\n",
    "def compute_shape(image_shape, pyramid_levels):\n",
    "    \"\"\"Compute shapes based on pyramid levels.\n",
    "    :param image_shape:\n",
    "    :param pyramid_levels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image_shape = np.array(image_shape[:2])\n",
    "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
    "    return image_shapes\n",
    "\n",
    "\n",
    "def anchors_for_shape(\n",
    "    image_shape,\n",
    "    pyramid_levels=None,\n",
    "    ratios=None,\n",
    "    scales=None,\n",
    "    strides=None,\n",
    "    sizes=None,\n",
    "    shapes_callback=None,\n",
    "):\n",
    "\n",
    "    image_shapes = compute_shape(image_shape, pyramid_levels)\n",
    "\n",
    "    # compute anchors over all pyramid levels\n",
    "    all_anchors = np.zeros((0, 4))\n",
    "    for idx, p in enumerate(pyramid_levels):\n",
    "        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n",
    "        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n",
    "        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "\n",
    "def shift(shape, stride, anchors):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel(),\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    A = anchors.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "    all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "    return all_anchors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86dc0aa1-786d-4991-b3ee-b491320ebd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import skimage.color\n",
    "import skimage\n",
    "from PIL import Image\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    \"\"\"CSV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train_file, class_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_file (string): CSV file with training annotations\n",
    "            annotations (string): CSV file with class list\n",
    "            test_file (string, optional): CSV file with testing annotations\n",
    "        \"\"\"\n",
    "        self.train_file = train_file\n",
    "        self.class_list = class_list\n",
    "        self.transform = transform\n",
    "\n",
    "        # parse the provided class file\n",
    "        try:\n",
    "            with self._open_for_csv(self.class_list) as file:\n",
    "                self.classes = self.load_classes(csv.reader(file, delimiter=','))\n",
    "        except ValueError as e:\n",
    "            raise(ValueError('invalid CSV class file: {}: {}'.format(self.class_list, e)))\n",
    "\n",
    "        self.labels = {}\n",
    "        for key, value in self.classes.items():\n",
    "            self.labels[value] = key\n",
    "        print(self.labels)\n",
    "        # csv with img_path, x1, y1, x2, y2, class_name\n",
    "        try:\n",
    "            with self._open_for_csv(self.train_file) as file:\n",
    "                self.image_data = self._read_annotations(csv.reader(file, delimiter=','), self.classes)\n",
    "        except ValueError as e:\n",
    "            raise(ValueError('invalid CSV annotations file: {}: {}'.format(self.train_file, e)))\n",
    "        self.image_names = list(self.image_data.keys())\n",
    "\n",
    "    def _parse(self, value, function, fmt):\n",
    "        \"\"\"\n",
    "        Parse a string into a value, and format a nice ValueError if it fails.\n",
    "        Returns `function(value)`.\n",
    "        Any `ValueError` raised is catched and a new `ValueError` is raised\n",
    "        with message `fmt.format(e)`, where `e` is the caught `ValueError`.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return function(value)\n",
    "        except ValueError as e:\n",
    "            raise(ValueError(fmt.format(e)), None)\n",
    "\n",
    "    def _open_for_csv(self, path):\n",
    "        \"\"\"\n",
    "        Open a file with flags suitable for csv.reader.\n",
    "        This is different for python2 it means with mode 'rb',\n",
    "        for python3 this means 'r' with \"universal newlines\".\n",
    "        \"\"\"\n",
    "        if sys.version_info[0] < 3:\n",
    "            return open(path, 'rb',encoding='utf-8-sig')\n",
    "        else:\n",
    "            return open(path, 'r',encoding='utf-8-sig',newline='')\n",
    "\n",
    "    def load_classes(self, csv_reader):\n",
    "        result = {}\n",
    "\n",
    "        for line, row in enumerate(csv_reader):\n",
    "            line += 1\n",
    "\n",
    "            try:\n",
    "                class_name, class_id = row\n",
    "            except ValueError:\n",
    "                raise(ValueError('line {}: format should be \\'class_name,class_id\\''.format(line)))\n",
    "            class_id = self._parse(class_id, int, 'line {}: malformed class ID: {{}}'.format(line))\n",
    "\n",
    "            if class_name in result:\n",
    "                raise ValueError('line {}: duplicate class name: \\'{}\\''.format(line, class_name))\n",
    "            result[class_name] = class_id\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.load_image(idx)\n",
    "        annot = self.load_annotations(idx)\n",
    "        sample = {'img': img, 'annot': annot}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        img = skimage.io.imread(self.image_names[image_index])\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = skimage.color.gray2rgb(img)\n",
    "\n",
    "        return img.astype(np.float32)/255.0\n",
    "\n",
    "    def load_annotations(self, image_index):\n",
    "        # get ground truth annotations\n",
    "        annotation_list = self.image_data[self.image_names[image_index]]\n",
    "        annotations     = np.zeros((0, 5))\n",
    "\n",
    "        # some images appear to miss annotations (like image with id 257034)\n",
    "        if len(annotation_list) == 0:\n",
    "            return annotations\n",
    "\n",
    "        # parse annotations\n",
    "        for idx, a in enumerate(annotation_list):\n",
    "            # some annotations have basically no width / height, skip them\n",
    "            x1 = a['x1']\n",
    "            x2 = a['x2']\n",
    "            y1 = a['y1']\n",
    "            y2 = a['y2']\n",
    "\n",
    "            if (x2-x1) < 1 or (y2-y1) < 1:\n",
    "                continue\n",
    "\n",
    "            annotation        = np.zeros((1, 5))\n",
    "            \n",
    "            annotation[0, 0] = x1\n",
    "            annotation[0, 1] = y1\n",
    "            annotation[0, 2] = x2\n",
    "            annotation[0, 3] = y2\n",
    "\n",
    "            annotation[0, 4]  = self.name_to_label(a['class'])\n",
    "            annotations       = np.append(annotations, annotation, axis=0)\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def _read_annotations(self, csv_reader, classes):\n",
    "        result = {}\n",
    "        for line, row in enumerate(csv_reader):\n",
    "            line += 1\n",
    "\n",
    "            try:\n",
    "                img_file, x1, y1, x2, y2, class_name = row[:6]\n",
    "            except ValueError:\n",
    "                raise(ValueError('line {}: format should be \\'img_file,x1,y1,x2,y2,class_name\\' or \\'img_file,,,,,\\''.format(line)), None)\n",
    "\n",
    "            if img_file not in result:\n",
    "                result[img_file] = []\n",
    "\n",
    "            # If a row contains only an image path, it's an image without annotations.\n",
    "            if (x1, y1, x2, y2, class_name) == ('', '', '', '', ''):\n",
    "                continue\n",
    "\n",
    "            x1 = self._parse(x1, float, 'line {}: malformed x1: {{}}'.format(line))\n",
    "            y1 = self._parse(y1, float, 'line {}: malformed y1: {{}}'.format(line))\n",
    "            x2 = self._parse(x2, float, 'line {}: malformed x2: {{}}'.format(line))\n",
    "            y2 = self._parse(y2, float, 'line {}: malformed y2: {{}}'.format(line))\n",
    "\n",
    "            # Check that the bounding box is valid.\n",
    "            if x2 <= x1:\n",
    "                raise ValueError('line {}: x2 ({}) must be higher than x1 ({})'.format(line, x2, x1))\n",
    "            if y2 <= y1:\n",
    "                raise ValueError('line {}: y2 ({}) must be higher than y1 ({})'.format(line, y2, y1))\n",
    "\n",
    "            # check if the current class name is correctly present\n",
    "            if class_name not in classes:\n",
    "                raise ValueError('line {}: unknown class name: \\'{}\\' (classes: {})'.format(line, class_name, classes))\n",
    "\n",
    "            result[img_file].append({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': class_name})\n",
    "        return result\n",
    "\n",
    "    def name_to_label(self, name):\n",
    "        return self.classes[name]\n",
    "\n",
    "    def label_to_name(self, label):\n",
    "        return self.labels[label]\n",
    "\n",
    "    def num_classes(self):\n",
    "        return 2\n",
    "#         return max(self.classes.values()) + 1\n",
    "\n",
    "    def image_aspect_ratio(self, image_index):\n",
    "        image = Image.open(self.image_names[image_index])\n",
    "        return float(image.width) / float(image.height)\n",
    "    \n",
    "\n",
    "def collater(data):\n",
    "\n",
    "    imgs = [s['img'] for s in data]\n",
    "    annots = [s['annot'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "        \n",
    "    widths = [int(s.shape[0]) for s in imgs]\n",
    "    heights = [int(s.shape[1]) for s in imgs]\n",
    "    batch_size = len(imgs)\n",
    "\n",
    "    max_width = np.array(widths).max()\n",
    "    max_height = np.array(heights).max()\n",
    "\n",
    "    padded_imgs = torch.zeros(batch_size, max_width, max_height, 3)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = imgs[i]\n",
    "        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "    \n",
    "    if max_num_annots > 0:\n",
    "\n",
    "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        if max_num_annots > 0:\n",
    "            for idx, annot in enumerate(annots):\n",
    "                #print(annot.shape)\n",
    "                if annot.shape[0] > 0:\n",
    "                    annot_padded[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "\n",
    "    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': padded_imgs, 'annot': annot_padded, 'scale': scales}\n",
    "\n",
    "class Resizer(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, min_side=608, max_side=1024):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "\n",
    "        rows, cols, cns = image.shape\n",
    "\n",
    "        smallest_side = min(rows, cols)\n",
    "\n",
    "        # rescale the image so the smallest side is min_side\n",
    "        scale = min_side / smallest_side\n",
    "\n",
    "        # check if the largest side is now greater than max_side, which can happen\n",
    "        # when images have a large aspect ratio\n",
    "        largest_side = max(rows, cols)\n",
    "\n",
    "        if largest_side * scale > max_side:\n",
    "            scale = max_side / largest_side\n",
    "\n",
    "        # resize the image with the computed scale\n",
    "        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))\n",
    "        rows, cols, cns = image.shape\n",
    "\n",
    "        pad_w = 32 - rows%32\n",
    "        pad_h = 32 - cols%32\n",
    "\n",
    "        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n",
    "        new_image[:rows, :cols, :] = image.astype(np.float32)\n",
    "\n",
    "        annots[:, :4] *= scale\n",
    "\n",
    "        return {'img': torch.from_numpy(new_image), 'annot': torch.from_numpy(annots), 'scale': scale}\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, flip_x=0.5):\n",
    "\n",
    "        if np.random.rand() < flip_x:\n",
    "            image, annots = sample['img'], sample['annot']\n",
    "            image = image[:, ::-1, :]\n",
    "\n",
    "            rows, cols, channels = image.shape\n",
    "\n",
    "            x1 = annots[:, 0].copy()\n",
    "            x2 = annots[:, 2].copy()\n",
    "            \n",
    "            x_tmp = x1.copy()\n",
    "\n",
    "            annots[:, 0] = cols - x2\n",
    "            annots[:, 2] = cols - x_tmp\n",
    "\n",
    "            sample = {'img': image, 'annot': annots}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean = np.array([[[0.485, 0.456, 0.406]]])\n",
    "        self.std = np.array([[[0.229, 0.224, 0.225]]])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "\n",
    "        return {'img':((image.astype(np.float32)-self.mean)/self.std), 'annot': annots}\n",
    "\n",
    "class UnNormalizer(object):\n",
    "    def __init__(self, mean=None, std=None):\n",
    "        if mean == None:\n",
    "            self.mean = [0.485, 0.456, 0.406]\n",
    "        else:\n",
    "            self.mean = mean\n",
    "        if std == None:\n",
    "            self.std = [0.229, 0.224, 0.225]\n",
    "        else:\n",
    "            self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class AspectRatioBasedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size, drop_last):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.groups = self.group_images()\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.groups)\n",
    "        for group in self.groups:\n",
    "            yield group\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.data_source) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def group_images(self):\n",
    "        # determine the order of the images\n",
    "        order = list(range(len(self.data_source)))\n",
    "        order.sort(key=lambda x: self.data_source.image_aspect_ratio(x))\n",
    "\n",
    "        # divide into groups, one group = one batch\n",
    "        return [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdd527e-b8d7-41bc-a575-6363fae00baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def calc_iou(a, b):\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n",
    "    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = torch.clamp(iw, min=0)\n",
    "    ih = torch.clamp(ih, min=0)\n",
    "\n",
    "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
    "\n",
    "    ua = torch.clamp(ua, min=1e-8)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    IoU = intersection / ua\n",
    "\n",
    "    return IoU\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    #def __init__(self):\n",
    "\n",
    "    def forward(self, classifications, regressions, anchors, annotations):\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        batch_size = classifications.shape[0]\n",
    "        classification_losses = []\n",
    "        regression_losses = []\n",
    "\n",
    "        anchor = anchors[0, :, :]\n",
    "\n",
    "        anchor_widths  = anchor[:, 2] - anchor[:, 0]\n",
    "        anchor_heights = anchor[:, 3] - anchor[:, 1]\n",
    "        anchor_ctr_x   = anchor[:, 0] + 0.5 * anchor_widths\n",
    "        anchor_ctr_y   = anchor[:, 1] + 0.5 * anchor_heights\n",
    "\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            classification = classifications[j, :, :]\n",
    "            regression = regressions[j, :, :]\n",
    "\n",
    "            bbox_annotation = annotations[j, :, :]\n",
    "            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n",
    "\n",
    "            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n",
    "\n",
    "            if bbox_annotation.shape[0] == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    alpha_factor = torch.ones(classification.shape).cuda() * alpha\n",
    "\n",
    "                    alpha_factor = 1. - alpha_factor\n",
    "                    focal_weight = classification\n",
    "                    focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
    "\n",
    "                    bce = -(torch.log(1.0 - classification))\n",
    "\n",
    "                    # cls_loss = focal_weight * torch.pow(bce, gamma)\n",
    "                    cls_loss = focal_weight * bce\n",
    "                    classification_losses.append(cls_loss.sum())\n",
    "                    regression_losses.append(torch.tensor(0).float().cuda())\n",
    "\n",
    "                else:\n",
    "                    alpha_factor = torch.ones(classification.shape) * alpha\n",
    "\n",
    "                    alpha_factor = 1. - alpha_factor\n",
    "                    focal_weight = classification\n",
    "                    focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
    "\n",
    "                    bce = -(torch.log(1.0 - classification))\n",
    "\n",
    "                    # cls_loss = focal_weight * torch.pow(bce, gamma)\n",
    "                    cls_loss = focal_weight * bce\n",
    "                    classification_losses.append(cls_loss.sum())\n",
    "                    regression_losses.append(torch.tensor(0).float())\n",
    "\n",
    "                continue\n",
    "\n",
    "            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations\n",
    "\n",
    "            IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1\n",
    "\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            # compute the loss for classification\n",
    "            targets = torch.ones(classification.shape) * -1\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            targets[torch.lt(IoU_max, 0.4), :] = 0\n",
    "\n",
    "            positive_indices = torch.ge(IoU_max, 0.5)\n",
    "\n",
    "            num_positive_anchors = positive_indices.sum()\n",
    "\n",
    "            assigned_annotations = bbox_annotation[IoU_argmax, :]\n",
    "\n",
    "            targets[positive_indices, :] = 0\n",
    "            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                alpha_factor = torch.ones(targets.shape).cuda() * alpha\n",
    "            else:\n",
    "                alpha_factor = torch.ones(targets.shape) * alpha\n",
    "\n",
    "            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n",
    "            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n",
    "            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
    "\n",
    "            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n",
    "\n",
    "            # cls_loss = focal_weight * torch.pow(bce, gamma)\n",
    "            cls_loss = focal_weight * bce\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda())\n",
    "            else:\n",
    "                cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape))\n",
    "\n",
    "            classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0))\n",
    "\n",
    "            # compute the loss for regression\n",
    "\n",
    "            if positive_indices.sum() > 0:\n",
    "                assigned_annotations = assigned_annotations[positive_indices, :]\n",
    "\n",
    "                anchor_widths_pi = anchor_widths[positive_indices]\n",
    "                anchor_heights_pi = anchor_heights[positive_indices]\n",
    "                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n",
    "                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n",
    "\n",
    "                gt_widths  = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n",
    "                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n",
    "                gt_ctr_x   = assigned_annotations[:, 0] + 0.5 * gt_widths\n",
    "                gt_ctr_y   = assigned_annotations[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "                # clip widths to 1\n",
    "                gt_widths  = torch.clamp(gt_widths, min=1)\n",
    "                gt_heights = torch.clamp(gt_heights, min=1)\n",
    "\n",
    "                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n",
    "                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n",
    "                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n",
    "                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n",
    "\n",
    "                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))\n",
    "                targets = targets.t()\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda()\n",
    "                else:\n",
    "                    targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]])\n",
    "\n",
    "                negative_indices = 1 + (~positive_indices)\n",
    "\n",
    "                regression_diff = torch.abs(targets - regression[positive_indices, :])\n",
    "\n",
    "                regression_loss = torch.where(\n",
    "                    torch.le(regression_diff, 1.0 / 9.0),\n",
    "                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n",
    "                    regression_diff - 0.5 / 9.0\n",
    "                )\n",
    "                regression_losses.append(regression_loss.mean())\n",
    "            else:\n",
    "                if torch.cuda.is_available():\n",
    "                    regression_losses.append(torch.tensor(0).float().cuda())\n",
    "                else:\n",
    "                    regression_losses.append(torch.tensor(0).float())\n",
    "\n",
    "        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8a0c78-e620-45a8-90dd-08d8d9fe8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision.ops import nms\n",
    "# from retinanet.utils import BasicBlock, Bottleneck, BBoxTransform, ClipBoxes\n",
    "# from retinanet.anchors import Anchors\n",
    "# from retinanet import losses\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "        self.P7_1 = nn.ReLU()\n",
    "        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "\n",
    "        P3_x = self.P3_1(C3)\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "\n",
    "        P6_x = self.P6(C5)\n",
    "\n",
    "        P7_x = self.P7_1(P6_x)\n",
    "        P7_x = self.P7_2(P7_x)\n",
    "\n",
    "        return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = 4*num_anchors\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 4)\n",
    "\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        out = self.output_act(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = n_classes + n_anchors\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, block, layers):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        if block == BasicBlock:\n",
    "            fpn_sizes = [self.layer2[layers[1] - 1].conv2.out_channels, self.layer3[layers[2] - 1].conv2.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv2.out_channels]\n",
    "        elif block == Bottleneck:\n",
    "            fpn_sizes = [self.layer2[layers[1] - 1].conv3.out_channels, self.layer3[layers[2] - 1].conv3.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv3.out_channels]\n",
    "        else:\n",
    "            raise ValueError(f\"Block type {block} not understood\")\n",
    "\n",
    "        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2])\n",
    "\n",
    "        self.regressionModel = RegressionModel(256)\n",
    "        self.classificationModel = ClassificationModel(256, num_classes=num_classes)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "\n",
    "        self.focalLoss = FocalLoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        prior = 0.01\n",
    "\n",
    "        self.classificationModel.output.weight.data.fill_(0)\n",
    "        self.classificationModel.output.bias.data.fill_(-math.log((1.0 - prior) / prior))\n",
    "\n",
    "        self.regressionModel.output.weight.data.fill_(0)\n",
    "        self.regressionModel.output.bias.data.fill_(0)\n",
    "\n",
    "        self.freeze_bn()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if self.training:\n",
    "            img_batch, annotations = inputs\n",
    "        else:\n",
    "            img_batch = inputs\n",
    "\n",
    "        x = self.conv1(img_batch)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        features = self.fpn([x2, x3, x4])\n",
    "\n",
    "        regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n",
    "\n",
    "        classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n",
    "\n",
    "        anchors = self.anchors(img_batch)\n",
    "\n",
    "        if self.training:\n",
    "            return self.focalLoss(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n",
    "\n",
    "            finalResult = [[], [], []]\n",
    "\n",
    "            finalScores = torch.Tensor([])\n",
    "            finalAnchorBoxesIndexes = torch.Tensor([]).long()\n",
    "            finalAnchorBoxesCoordinates = torch.Tensor([])\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                finalScores = finalScores.cuda()\n",
    "                finalAnchorBoxesIndexes = finalAnchorBoxesIndexes.cuda()\n",
    "                finalAnchorBoxesCoordinates = finalAnchorBoxesCoordinates.cuda()\n",
    "\n",
    "            for i in range(classification.shape[2]):\n",
    "                scores = torch.squeeze(classification[:, :, i])\n",
    "                scores_over_thresh = (scores > 0.05)\n",
    "                if scores_over_thresh.sum() == 0:\n",
    "                    # no boxes to NMS, just continue\n",
    "                    continue\n",
    "\n",
    "                scores = scores[scores_over_thresh]\n",
    "                anchorBoxes = torch.squeeze(transformed_anchors)\n",
    "                anchorBoxes = anchorBoxes[scores_over_thresh]\n",
    "                anchors_nms_idx = nms(anchorBoxes, scores, 0.5)\n",
    "\n",
    "                finalResult[0].extend(scores[anchors_nms_idx])\n",
    "                finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0]))\n",
    "                finalResult[2].extend(anchorBoxes[anchors_nms_idx])\n",
    "\n",
    "                finalScores = torch.cat((finalScores, scores[anchors_nms_idx]))\n",
    "                finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0])\n",
    "                if torch.cuda.is_available():\n",
    "                    finalAnchorBoxesIndexesValue = finalAnchorBoxesIndexesValue.cuda()\n",
    "\n",
    "                finalAnchorBoxesIndexes = torch.cat((finalAnchorBoxesIndexes, finalAnchorBoxesIndexesValue))\n",
    "                finalAnchorBoxesCoordinates = torch.cat((finalAnchorBoxesCoordinates, anchorBoxes[anchors_nms_idx]))\n",
    "\n",
    "            return [finalScores, finalAnchorBoxesIndexes, finalAnchorBoxesCoordinates]\n",
    "\n",
    "\n",
    "\n",
    "def resnet18(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], model_dir='.'), strict=False)\n",
    "        model.fc = nn.Linear(2048,2)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152'], model_dir='.'), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762fa2b4-b25d-49ec-9a20-848550fa38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "Calcu_IoU:\n",
    "    input:anchors, annotations\n",
    "    output:a matrix in size of anchor_num * annotation_num\n",
    "    purpose: Calcu all possible matches' IoU\n",
    "\"\"\"\n",
    "def Calcu_IoU(anchors, annotations):\n",
    "    annotation_area = (annotations[:, 2] - annotations[:, 0]) * (annotations[:, 3] - annotations[:, 1])\n",
    "\n",
    "    intersectant_width = torch.min(torch.unsqueeze(anchors[:, 2], dim=1), annotations[:, 2]) - torch.max(torch.unsqueeze(anchors[:, 0], 1), annotations[:, 0])\n",
    "    intersectant_height = torch.min(torch.unsqueeze(anchors[:, 3], dim=1), annotations[:, 3]) - torch.max(torch.unsqueeze(anchors[:, 1], 1), annotations[:, 1])\n",
    "\n",
    "    intersectant_width = torch.clamp(intersectant_width, min=0)\n",
    "    intersectant_height = torch.clamp(intersectant_height, min=0)\n",
    "\n",
    "    union_area = torch.unsqueeze((anchors[:, 2] - anchors[:, 0]) * (anchors[:, 3] - anchors[:, 1]), dim=1) + annotation_area - intersectant_width * intersectant_height\n",
    "\n",
    "    union_area = torch.clamp(union_area, min=1e-8)\n",
    "\n",
    "    intersection = intersectant_width * intersectant_height\n",
    "\n",
    "    IoU = intersection / union_area\n",
    "\n",
    "    return IoU\n",
    "\n",
    "\n",
    "def Calcu_Loss(regressions, classifications, anchors, annotations):\n",
    "    alpha = 0.25\n",
    "    gamma = 2.0\n",
    "    classification_loss = []\n",
    "    regression_loss = []\n",
    "    anchor = anchors[0, :, :].cuda()\n",
    "    batch_size = classifications.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \"\"\"\n",
    "        part 1:\n",
    "        calcu classification loss\n",
    "        formula: Classification Loss(p) = -alpha * (1 - p) ** gamma * log(p)\n",
    "        \"\"\"\n",
    "        classification = classifications[i, :, :]\n",
    "        annotation = annotations[i, :, :].cuda()\n",
    "        #throw empty annotations if they exist\n",
    "        annotation = annotation[annotation[:, 4] != -1]\n",
    "\n",
    "        if annotation.shape[0] == 0:\n",
    "            regression_loss.append(torch.tensor(0).float().cuda())\n",
    "            classification_loss.append(torch.tensor(0).float().cuda())\n",
    "            continue\n",
    "\n",
    "        classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n",
    "        IoU = Calcu_IoU(anchor, annotation[:, :4])\n",
    "        IoU_max, annotation_index = torch.max(IoU, dim=1)\n",
    "\n",
    "        \"\"\"\n",
    "        setup judge matrix\n",
    "        if an anchor is positive, the matrix will puts 1 on the possible annotation position\n",
    "        if an anchor is negtive, the matrix will puts 0s on the whole line\n",
    "        otherwise, the matrix will puts -1s on the whole line\n",
    "        \"\"\"\n",
    "        judge_matrix = torch.ones(classification.shape) * -1\n",
    "        judge_matrix = judge_matrix.cuda()\n",
    "\n",
    "        judge_matrix[torch.lt(IoU_max, 0.4), :] = 0\n",
    "\n",
    "        valid_annotation = annotation[annotation_index, :]\n",
    "        positive_indices = torch.ge(IoU_max, 0.5)\n",
    "\n",
    "        judge_matrix[positive_indices, :] = 0\n",
    "        judge_matrix[positive_indices, valid_annotation[positive_indices, 4].long()] = 1\n",
    "\n",
    "        alpha_matrix = torch.ones(judge_matrix.shape).cuda() * alpha\n",
    "        alpha_matrix = torch.where(torch.eq(judge_matrix, 1.), alpha_matrix, 1. - alpha_matrix)\n",
    "\n",
    "        \"\"\"\n",
    "        setup possibility matrix ans calcu the '-alpah * (1 - p) ** gamma' part\n",
    "        \"\"\"\n",
    "        possibility_matrix = torch.where(torch.eq(judge_matrix, 1.), 1. - classification, classification)\n",
    "        possibility_matrix = alpha_matrix * torch.pow(possibility_matrix, gamma)\n",
    "\n",
    "        \"\"\"\n",
    "        do the rest part and put the loss into the final output classification loss\n",
    "        \"\"\"\n",
    "        cl = -(judge_matrix * torch.log(possibility_matrix) + (1.0 - judge_matrix) * torch.log(1.0 - possibility_matrix))\n",
    "        cl = possibility_matrix * cl\n",
    "        cl = torch.where(torch.ne(judge_matrix, -1.0), cl, torch.zeros(cl.shape).cuda())\n",
    "        classification_loss.append(cl.sum() / torch.clamp(positive_indices.sum().float(), min=1.0))\n",
    "\n",
    "        \"\"\"\n",
    "        part 2:\n",
    "        calcu the regression loss\n",
    "        formula: Regression Loss(regression) = Smooth(destiny regression - regression)\n",
    "                 Smooth(x) = 0.5 * x ** 2 (abs(x) <= 1) or abs(x) - 0.5 (abs(x)>1)\n",
    "        \"\"\"\n",
    "        if positive_indices.sum() > 0:\n",
    "            positive_annotation = valid_annotation[positive_indices, :]\n",
    "            anchor_widths = anchor[:, 2] - anchor[:, 0]\n",
    "            anchor_heights = anchor[:, 3] - anchor[:, 1]\n",
    "            anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths\n",
    "            anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights\n",
    "\n",
    "            positive_anchor_widths = anchor_widths[positive_indices]\n",
    "            positive_anchor_heights = anchor_heights[positive_indices]\n",
    "            positive_anchor_ctr_x = anchor_ctr_x[positive_indices]\n",
    "            positive_anchor_ctr_y = anchor_ctr_y[positive_indices]\n",
    "\n",
    "            ground_truth_widths = positive_annotation[:, 2] - positive_annotation[:, 0]\n",
    "            ground_truth_heights = positive_annotation[:, 3] - positive_annotation[:, 1]\n",
    "            ground_truth_ctr_x = positive_annotation[:, 0] + 0.5 * ground_truth_widths\n",
    "            ground_truth_ctr_y = positive_annotation[:, 1] + 0.5 * ground_truth_heights\n",
    "\n",
    "            #prevent the ground truth's size is too small\n",
    "            torch.clamp(ground_truth_widths, min=1)\n",
    "            torch.clamp(ground_truth_heights, min=1)\n",
    "\n",
    "            #calcu the destiny regression\n",
    "            destiny_regression_x = (ground_truth_ctr_x - positive_anchor_ctr_x) / positive_anchor_widths\n",
    "            destiny_regression_y = (ground_truth_ctr_y - positive_anchor_ctr_y) / positive_anchor_heights\n",
    "            destiny_regression_widths = torch.log(ground_truth_widths / positive_anchor_widths)\n",
    "            destiny_regression_heights = torch.log(ground_truth_heights / positive_anchor_heights)\n",
    "\n",
    "            destiny_regression = torch.stack((destiny_regression_x, destiny_regression_y, destiny_regression_widths, destiny_regression_heights))\n",
    "            destiny_regression = destiny_regression.t()\n",
    "\n",
    "            regression =regressions[i, :, :]\n",
    "            destiny_regression = destiny_regression / torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda()\n",
    "            regression_delta = torch.abs(destiny_regression - regression[positive_indices, :])\n",
    "            #since a group of 9 anchors share the same center, the parameters in function Smooth need to divide into 9\n",
    "            rl = torch.where(torch.le(regression_delta, 1.0 / 9.0), 0.5 * torch.pow(regression_delta, 2), regression_delta - 0.5 / 9.0)\n",
    "            regression_loss.append(rl.mean())\n",
    "        else:\n",
    "            regression_loss.append(torch.tensor(0).float().cuda())\n",
    "\n",
    "    return torch.stack(classification_loss).mean(dim=0, keepdim=True), torch.stack(regression_loss).mean(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c636c1-2f17-446e-8e01-b289817c462b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb5df0b-e970-4899-a40a-8243abcbb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BBoxTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, mean=None, std=None):\n",
    "        super(BBoxTransform, self).__init__()\n",
    "        if mean is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32)).cuda()\n",
    "            else:\n",
    "                self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32))\n",
    "\n",
    "        else:\n",
    "            self.mean = mean\n",
    "        if std is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32)).cuda()\n",
    "            else:\n",
    "                self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))\n",
    "        else:\n",
    "            self.std = std\n",
    "\n",
    "    def forward(self, boxes, deltas):\n",
    "\n",
    "        widths  = boxes[:, :, 2] - boxes[:, :, 0]\n",
    "        heights = boxes[:, :, 3] - boxes[:, :, 1]\n",
    "        ctr_x   = boxes[:, :, 0] + 0.5 * widths\n",
    "        ctr_y   = boxes[:, :, 1] + 0.5 * heights\n",
    "\n",
    "        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n",
    "        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n",
    "        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n",
    "        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n",
    "\n",
    "        pred_ctr_x = ctr_x + dx * widths\n",
    "        pred_ctr_y = ctr_y + dy * heights\n",
    "        pred_w     = torch.exp(dw) * widths\n",
    "        pred_h     = torch.exp(dh) * heights\n",
    "\n",
    "        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n",
    "        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n",
    "        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n",
    "        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "\n",
    "class ClipBoxes(nn.Module):\n",
    "\n",
    "    def __init__(self, width=None, height=None):\n",
    "        super(ClipBoxes, self).__init__()\n",
    "\n",
    "    def forward(self, boxes, img):\n",
    "\n",
    "        batch_size, num_channels, height, width = img.shape\n",
    "\n",
    "        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n",
    "        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n",
    "\n",
    "        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n",
    "        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n",
    "      \n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1170f8-b53a-4a3a-a2bc-654f9eb5dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: (N, 4) ndarray of float\n",
    "    b: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
    "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua\n",
    "\n",
    "\n",
    "def _compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def _get_detections(dataset, retinanet, score_threshold=0.05, max_detections=100, save_path=None):\n",
    "    \"\"\" Get the detections from the retinanet using the generator.\n",
    "    The result is a list of lists such that the size is:\n",
    "        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n",
    "    # Arguments\n",
    "        dataset         : The generator used to run images through the retinanet.\n",
    "        retinanet           : The retinanet to run on the images.\n",
    "        score_threshold : The score confidence threshold to use.\n",
    "        max_detections  : The maximum number of detections to use per image.\n",
    "        save_path       : The path to save the images with visualized detections to.\n",
    "    # Returns\n",
    "        A list of lists containing the detections for each image in the generator.\n",
    "    \"\"\"\n",
    "    all_detections = [[None for i in range(dataset.num_classes())] for j in range(len(dataset))]\n",
    "\n",
    "    retinanet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for index in range(len(dataset)):\n",
    "            data = dataset[index]\n",
    "            scale = data['scale']\n",
    "\n",
    "            # run network\n",
    "            if torch.cuda.is_available():\n",
    "                scores, labels, boxes = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "            else:\n",
    "                scores, labels, boxes = retinanet(data['img'].permute(2, 0, 1).float().unsqueeze(dim=0))\n",
    "            scores = scores.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            boxes  = boxes.cpu().numpy()\n",
    "\n",
    "            # correct boxes for image scale\n",
    "            boxes /= scale\n",
    "\n",
    "            # select indices which have a score above the threshold\n",
    "            indices = np.where(scores > score_threshold)[0]\n",
    "            if indices.shape[0] > 0:\n",
    "                # select those scores\n",
    "                scores = scores[indices]\n",
    "\n",
    "                # find the order with which to sort the scores\n",
    "                scores_sort = np.argsort(-scores)[:max_detections]\n",
    "\n",
    "                # select detections\n",
    "                image_boxes      = boxes[indices[scores_sort], :]\n",
    "                image_scores     = scores[scores_sort]\n",
    "                image_labels     = labels[indices[scores_sort]]\n",
    "                image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n",
    "\n",
    "                # copy detections to all_detections\n",
    "                for label in range(dataset.num_classes()):\n",
    "                    all_detections[index][label] = image_detections[image_detections[:, -1] == label, :-1]\n",
    "            else:\n",
    "                # copy detections to all_detections\n",
    "                for label in range(dataset.num_classes()):\n",
    "                    all_detections[index][label] = np.zeros((0, 5))\n",
    "\n",
    "            print('{}/{}'.format(index + 1, len(dataset)), end='\\r')\n",
    "\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "def _get_annotations(generator):\n",
    "    \"\"\" Get the ground truth annotations from the generator.\n",
    "    The result is a list of lists such that the size is:\n",
    "        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n",
    "    # Arguments\n",
    "        generator : The generator used to retrieve ground truth annotations.\n",
    "    # Returns\n",
    "        A list of lists containing the annotations for each image in the generator.\n",
    "    \"\"\"\n",
    "    all_annotations = [[None for i in range(generator.num_classes())] for j in range(len(generator))]\n",
    "\n",
    "    for i in range(len(generator)):\n",
    "        # load the annotations\n",
    "        annotations = generator.load_annotations(i)\n",
    "\n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "\n",
    "        print('{}/{}'.format(i + 1, len(generator)), end='\\r')\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    generator,\n",
    "    retinanet,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.05,\n",
    "    max_detections=100,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\" Evaluate a given dataset using a given retinanet.\n",
    "    # Arguments\n",
    "        generator       : The generator that represents the dataset to evaluate.\n",
    "        retinanet           : The retinanet to evaluate.\n",
    "        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "        score_threshold : The score confidence threshold to use for detections.\n",
    "        max_detections  : The maximum number of detections to use per image.\n",
    "        save_path       : The path to save precision recall curve of each label.\n",
    "    # Returns\n",
    "        A dict mapping class names to mAP scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # gather all detections and annotations\n",
    "\n",
    "    all_detections     = _get_detections(generator, retinanet, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n",
    "    all_annotations    = _get_annotations(generator)\n",
    "\n",
    "    average_precisions = {}\n",
    "\n",
    "    for label in range(generator.num_classes()):\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(len(generator)):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    continue\n",
    "\n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0, 0\n",
    "            continue\n",
    "\n",
    "        # sort by score\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision\n",
    "        recall    = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = _compute_ap(recall, precision)\n",
    "        average_precisions[label] = average_precision, num_annotations\n",
    "\n",
    "\n",
    "    print('\\nmAP:')\n",
    "    print('generator.num_classes',generator.num_classes())\n",
    "#     print('generator.label_to_name',generator.label_to_name().keys())\n",
    "    for label in range(generator.num_classes()):\n",
    "\n",
    "#         if label==4 or label==8:\n",
    "        label_name = generator.label_to_name(label)\n",
    "#         else:\n",
    "#             continue\n",
    "        print('{}: {}'.format(label_name, average_precisions[label][0]))\n",
    "        print(\"Precision: \",precision[-1])\n",
    "        print(\"Recall: \",recall[-1])\n",
    "        \n",
    "        if save_path!=None:\n",
    "            plt.plot(recall,precision)\n",
    "            # naming the x axis \n",
    "            plt.xlabel('Recall') \n",
    "            # naming the y axis \n",
    "            plt.ylabel('Precision') \n",
    "\n",
    "            # giving a title to my graph \n",
    "            plt.title('Precision Recall curve') \n",
    "\n",
    "            # function to show the plot\n",
    "            plt.savefig(save_path+'/'+label_name+'_precision_recall.jpg')\n",
    "    return average_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4e4a5-975f-4ec4-8f85-bc620e161f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Simple training script for training a RetinaNet network.\n",
      "{0: '0', 1: '1'}\n",
      "{0: '0', 1: '1'}\n",
      "Num training images: 866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iteration: 0 | Classification loss: 1.13259 | Regression loss: 1.02125 | Running loss: 2.15384\n",
      "Epoch: 0 | Iteration: 1 | Classification loss: 0.61430 | Regression loss: 0.49084 | Running loss: 1.62949\n",
      "Epoch: 0 | Iteration: 2 | Classification loss: 1.10111 | Regression loss: 0.87361 | Running loss: 1.74457\n",
      "Epoch: 0 | Iteration: 3 | Classification loss: 1.19185 | Regression loss: 0.99993 | Running loss: 1.85637\n",
      "Epoch: 0 | Iteration: 4 | Classification loss: 1.18887 | Regression loss: 0.99156 | Running loss: 1.92118\n",
      "Epoch: 0 | Iteration: 5 | Classification loss: 1.08607 | Regression loss: 1.12560 | Running loss: 1.96960\n",
      "Epoch: 0 | Iteration: 6 | Classification loss: 1.16253 | Regression loss: 1.00597 | Running loss: 1.99801\n",
      "Epoch: 0 | Iteration: 7 | Classification loss: 1.16448 | Regression loss: 0.92947 | Running loss: 2.01000\n",
      "Epoch: 0 | Iteration: 8 | Classification loss: 1.09961 | Regression loss: 1.13997 | Running loss: 2.03551\n",
      "Epoch: 0 | Iteration: 9 | Classification loss: 1.14446 | Regression loss: 0.97568 | Running loss: 2.04398\n",
      "Epoch: 0 | Iteration: 10 | Classification loss: 0.99484 | Regression loss: 1.01027 | Running loss: 2.04044\n",
      "Epoch: 0 | Iteration: 11 | Classification loss: 1.04961 | Regression loss: 1.10978 | Running loss: 2.05036\n",
      "Epoch: 0 | Iteration: 12 | Classification loss: 0.96952 | Regression loss: 0.88717 | Running loss: 2.03546\n",
      "Epoch: 0 | Iteration: 13 | Classification loss: 0.69657 | Regression loss: 0.56773 | Running loss: 1.98038\n",
      "Epoch: 0 | Iteration: 14 | Classification loss: 1.10941 | Regression loss: 0.97356 | Running loss: 1.98722\n",
      "Epoch: 0 | Iteration: 15 | Classification loss: 1.02163 | Regression loss: 0.95292 | Running loss: 1.98642\n",
      "Epoch: 0 | Iteration: 16 | Classification loss: 0.97453 | Regression loss: 1.08552 | Running loss: 1.99075\n",
      "Epoch: 0 | Iteration: 17 | Classification loss: 1.13902 | Regression loss: 1.08512 | Running loss: 2.00372\n",
      "Epoch: 0 | Iteration: 18 | Classification loss: 0.96013 | Regression loss: 0.88260 | Running loss: 1.99525\n",
      "Epoch: 0 | Iteration: 19 | Classification loss: 0.66898 | Regression loss: 0.37112 | Running loss: 1.94749\n",
      "Epoch: 0 | Iteration: 20 | Classification loss: 0.87480 | Regression loss: 0.82834 | Running loss: 1.93585\n",
      "Epoch: 0 | Iteration: 21 | Classification loss: 0.94255 | Regression loss: 1.10787 | Running loss: 1.94106\n",
      "Epoch: 0 | Iteration: 22 | Classification loss: 1.15382 | Regression loss: 0.93579 | Running loss: 1.94752\n",
      "Epoch: 0 | Iteration: 23 | Classification loss: 0.79950 | Regression loss: 0.97333 | Running loss: 1.94024\n",
      "Epoch: 0 | Iteration: 24 | Classification loss: 1.06877 | Regression loss: 0.98950 | Running loss: 1.94496\n",
      "Epoch: 0 | Iteration: 25 | Classification loss: 0.86793 | Regression loss: 0.89647 | Running loss: 1.93802\n",
      "Epoch: 0 | Iteration: 26 | Classification loss: 0.81943 | Regression loss: 1.22460 | Running loss: 1.94194\n",
      "Epoch: 0 | Iteration: 27 | Classification loss: 1.01605 | Regression loss: 0.95636 | Running loss: 1.94303\n",
      "Epoch: 0 | Iteration: 28 | Classification loss: 0.77363 | Regression loss: 1.00302 | Running loss: 1.93729\n",
      "Epoch: 0 | Iteration: 29 | Classification loss: 0.98514 | Regression loss: 0.98022 | Running loss: 1.93823\n",
      "Epoch: 0 | Iteration: 30 | Classification loss: 1.00633 | Regression loss: 0.90845 | Running loss: 1.93747\n",
      "Epoch: 0 | Iteration: 31 | Classification loss: 0.73337 | Regression loss: 1.08879 | Running loss: 1.93387\n",
      "Epoch: 0 | Iteration: 32 | Classification loss: 0.93549 | Regression loss: 0.97555 | Running loss: 1.93318\n",
      "Epoch: 0 | Iteration: 33 | Classification loss: 0.99928 | Regression loss: 0.98552 | Running loss: 1.93470\n",
      "Epoch: 0 | Iteration: 34 | Classification loss: 0.88029 | Regression loss: 0.43213 | Running loss: 1.91692\n",
      "Epoch: 0 | Iteration: 35 | Classification loss: 1.14794 | Regression loss: 0.87117 | Running loss: 1.91976\n",
      "Epoch: 0 | Iteration: 36 | Classification loss: 1.00791 | Regression loss: 0.87148 | Running loss: 1.91866\n",
      "Epoch: 0 | Iteration: 37 | Classification loss: 0.79196 | Regression loss: 0.94594 | Running loss: 1.91391\n",
      "Epoch: 0 | Iteration: 38 | Classification loss: 0.98621 | Regression loss: 0.93252 | Running loss: 1.91403\n",
      "Epoch: 0 | Iteration: 39 | Classification loss: 0.87059 | Regression loss: 0.92206 | Running loss: 1.91100\n",
      "Epoch: 0 | Iteration: 40 | Classification loss: 0.83699 | Regression loss: 0.82975 | Running loss: 1.90504\n",
      "Epoch: 0 | Iteration: 41 | Classification loss: 0.86264 | Regression loss: 1.06465 | Running loss: 1.90557\n",
      "Epoch: 0 | Iteration: 42 | Classification loss: 0.70939 | Regression loss: 0.96052 | Running loss: 1.90009\n",
      "Epoch: 0 | Iteration: 43 | Classification loss: 0.88361 | Regression loss: 1.05552 | Running loss: 1.90098\n",
      "Epoch: 0 | Iteration: 44 | Classification loss: 0.81927 | Regression loss: 1.00201 | Running loss: 1.89920\n",
      "Epoch: 0 | Iteration: 45 | Classification loss: 1.01990 | Regression loss: 0.88721 | Running loss: 1.89938\n",
      "Epoch: 0 | Iteration: 46 | Classification loss: 0.77895 | Regression loss: 0.98373 | Running loss: 1.89647\n",
      "Epoch: 0 | Iteration: 47 | Classification loss: 0.80763 | Regression loss: 0.91895 | Running loss: 1.89293\n",
      "Epoch: 0 | Iteration: 48 | Classification loss: 0.79525 | Regression loss: 1.01398 | Running loss: 1.89122\n",
      "Epoch: 0 | Iteration: 49 | Classification loss: 0.88123 | Regression loss: 0.92125 | Running loss: 1.88945\n",
      "Epoch: 0 | Iteration: 50 | Classification loss: 0.90625 | Regression loss: 0.43255 | Running loss: 1.87865\n",
      "Epoch: 0 | Iteration: 51 | Classification loss: 1.01450 | Regression loss: 0.47228 | Running loss: 1.87111\n",
      "Epoch: 0 | Iteration: 52 | Classification loss: 0.81489 | Regression loss: 0.90780 | Running loss: 1.86831\n",
      "Epoch: 0 | Iteration: 53 | Classification loss: 0.82815 | Regression loss: 0.32115 | Running loss: 1.85500\n",
      "Epoch: 0 | Iteration: 54 | Classification loss: 0.87437 | Regression loss: 0.99787 | Running loss: 1.85531\n",
      "Epoch: 0 | Iteration: 55 | Classification loss: 0.92852 | Regression loss: 0.81901 | Running loss: 1.85339\n",
      "Epoch: 0 | Iteration: 56 | Classification loss: 0.92847 | Regression loss: 0.93217 | Running loss: 1.85351\n",
      "Epoch: 0 | Iteration: 57 | Classification loss: 1.00812 | Regression loss: 0.92523 | Running loss: 1.85489\n",
      "Epoch: 0 | Iteration: 58 | Classification loss: 0.89418 | Regression loss: 0.96909 | Running loss: 1.85503\n",
      "Epoch: 0 | Iteration: 59 | Classification loss: 0.76200 | Regression loss: 0.93238 | Running loss: 1.85235\n",
      "Epoch: 0 | Iteration: 60 | Classification loss: 0.81802 | Regression loss: 0.00000 | Running loss: 1.83540\n",
      "Epoch: 0 | Iteration: 61 | Classification loss: 0.72929 | Regression loss: 0.51239 | Running loss: 1.82582\n",
      "Epoch: 0 | Iteration: 62 | Classification loss: 0.99918 | Regression loss: 0.94885 | Running loss: 1.82776\n",
      "Epoch: 0 | Iteration: 63 | Classification loss: 0.87751 | Regression loss: 0.90366 | Running loss: 1.82703\n",
      "Epoch: 0 | Iteration: 64 | Classification loss: 0.80667 | Regression loss: 0.91057 | Running loss: 1.82535\n",
      "Epoch: 0 | Iteration: 65 | Classification loss: 0.74282 | Regression loss: 0.49192 | Running loss: 1.81640\n",
      "Epoch: 0 | Iteration: 66 | Classification loss: 0.85402 | Regression loss: 0.99088 | Running loss: 1.81682\n",
      "Epoch: 0 | Iteration: 67 | Classification loss: 1.02232 | Regression loss: 0.98599 | Running loss: 1.81964\n",
      "Epoch: 0 | Iteration: 68 | Classification loss: 0.67660 | Regression loss: 0.42996 | Running loss: 1.80930\n",
      "Epoch: 0 | Iteration: 69 | Classification loss: 1.10033 | Regression loss: 0.91656 | Running loss: 1.81227\n",
      "Epoch: 0 | Iteration: 70 | Classification loss: 1.03781 | Regression loss: 0.85402 | Running loss: 1.81339\n",
      "Epoch: 0 | Iteration: 71 | Classification loss: 0.74761 | Regression loss: 0.81901 | Running loss: 1.80996\n",
      "Epoch: 0 | Iteration: 72 | Classification loss: 0.79980 | Regression loss: 0.86753 | Running loss: 1.80801\n",
      "Epoch: 0 | Iteration: 73 | Classification loss: 0.81098 | Regression loss: 0.89516 | Running loss: 1.80663\n",
      "Epoch: 0 | Iteration: 74 | Classification loss: 0.61287 | Regression loss: 0.00000 | Running loss: 1.79071\n",
      "Epoch: 0 | Iteration: 75 | Classification loss: 1.28551 | Regression loss: 0.83360 | Running loss: 1.79504\n",
      "Epoch: 0 | Iteration: 76 | Classification loss: 0.84424 | Regression loss: 0.90627 | Running loss: 1.79446\n",
      "Epoch: 0 | Iteration: 77 | Classification loss: 0.33122 | Regression loss: 0.00000 | Running loss: 1.77570\n",
      "Epoch: 0 | Iteration: 78 | Classification loss: 0.97487 | Regression loss: 0.93589 | Running loss: 1.77741\n",
      "Epoch: 0 | Iteration: 79 | Classification loss: 0.85131 | Regression loss: 0.88870 | Running loss: 1.77694\n",
      "Epoch: 0 | Iteration: 80 | Classification loss: 1.03437 | Regression loss: 0.85827 | Running loss: 1.77837\n",
      "Epoch: 0 | Iteration: 81 | Classification loss: 0.76449 | Regression loss: 0.77481 | Running loss: 1.77545\n",
      "Epoch: 0 | Iteration: 82 | Classification loss: 1.04488 | Regression loss: 0.86160 | Running loss: 1.77703\n",
      "Epoch: 0 | Iteration: 83 | Classification loss: 0.80871 | Regression loss: 0.98921 | Running loss: 1.77728\n",
      "Epoch: 0 | Iteration: 84 | Classification loss: 0.82913 | Regression loss: 0.83321 | Running loss: 1.77593\n",
      "Epoch: 0 | Iteration: 85 | Classification loss: 0.99037 | Regression loss: 1.15463 | Running loss: 1.78022\n",
      "Epoch: 0 | Iteration: 86 | Classification loss: 0.80604 | Regression loss: 0.97444 | Running loss: 1.78022\n",
      "Epoch: 0 | Iteration: 87 | Classification loss: 0.79346 | Regression loss: 1.06577 | Running loss: 1.78112\n",
      "Epoch: 0 | Iteration: 88 | Classification loss: 1.02398 | Regression loss: 1.01270 | Running loss: 1.78399\n",
      "Epoch: 0 | Iteration: 89 | Classification loss: 0.86273 | Regression loss: 1.11402 | Running loss: 1.78613\n",
      "Epoch: 0 | Iteration: 90 | Classification loss: 0.92400 | Regression loss: 0.98317 | Running loss: 1.78746\n",
      "Epoch: 0 | Iteration: 91 | Classification loss: 0.85880 | Regression loss: 0.92352 | Running loss: 1.78741\n",
      "Epoch: 0 | Iteration: 92 | Classification loss: 0.96677 | Regression loss: 0.88415 | Running loss: 1.78809\n",
      "Epoch: 0 | Iteration: 93 | Classification loss: 1.57697 | Regression loss: 0.89371 | Running loss: 1.79535\n",
      "Epoch: 0 | Iteration: 94 | Classification loss: 0.73686 | Regression loss: 0.82562 | Running loss: 1.79290\n",
      "Epoch: 0 | Iteration: 95 | Classification loss: 0.72591 | Regression loss: 0.86781 | Running loss: 1.79083\n",
      "Epoch: 0 | Iteration: 96 | Classification loss: 0.69842 | Regression loss: 0.82449 | Running loss: 1.78806\n",
      "Epoch: 0 | Iteration: 97 | Classification loss: 0.76573 | Regression loss: 0.97291 | Running loss: 1.78756\n",
      "Epoch: 0 | Iteration: 98 | Classification loss: 0.84180 | Regression loss: 0.47358 | Running loss: 1.78279\n",
      "Epoch: 0 | Iteration: 99 | Classification loss: 0.93195 | Regression loss: 0.84768 | Running loss: 1.78276\n",
      "Epoch: 0 | Iteration: 100 | Classification loss: 0.74788 | Regression loss: 0.86141 | Running loss: 1.78104\n",
      "Epoch: 0 | Iteration: 101 | Classification loss: 0.72685 | Regression loss: 0.42359 | Running loss: 1.77486\n",
      "Epoch: 0 | Iteration: 102 | Classification loss: 0.82847 | Regression loss: 0.88771 | Running loss: 1.77429\n",
      "Epoch: 0 | Iteration: 103 | Classification loss: 0.67093 | Regression loss: 0.94041 | Running loss: 1.77272\n",
      "Epoch: 0 | Iteration: 104 | Classification loss: 0.94619 | Regression loss: 0.95725 | Running loss: 1.77397\n",
      "Epoch: 0 | Iteration: 105 | Classification loss: 0.96022 | Regression loss: 0.89408 | Running loss: 1.77473\n",
      "Epoch: 0 | Iteration: 106 | Classification loss: 0.94763 | Regression loss: 0.85876 | Running loss: 1.77502\n",
      "Epoch: 0 | Iteration: 107 | Classification loss: 0.66111 | Regression loss: 0.88806 | Running loss: 1.77293\n",
      "Epoch: 0 | Iteration: 108 | Classification loss: 0.83396 | Regression loss: 0.95803 | Running loss: 1.77310\n",
      "Epoch: 0 | Iteration: 109 | Classification loss: 0.68715 | Regression loss: 0.96769 | Running loss: 1.77203\n",
      "Epoch: 0 | Iteration: 110 | Classification loss: 0.65125 | Regression loss: 0.99892 | Running loss: 1.77093\n",
      "Epoch: 0 | Iteration: 111 | Classification loss: 0.95572 | Regression loss: 1.05595 | Running loss: 1.77308\n",
      "Epoch: 0 | Iteration: 112 | Classification loss: 0.93550 | Regression loss: 0.73206 | Running loss: 1.77215\n",
      "Epoch: 0 | Iteration: 113 | Classification loss: 0.95582 | Regression loss: 0.83380 | Running loss: 1.77230\n",
      "Epoch: 0 | Iteration: 114 | Classification loss: 0.74928 | Regression loss: 0.34359 | Running loss: 1.76639\n",
      "Epoch: 0 | Iteration: 115 | Classification loss: 1.22323 | Regression loss: 0.42400 | Running loss: 1.76537\n",
      "Epoch: 0 | Iteration: 116 | Classification loss: 1.17635 | Regression loss: 0.41386 | Running loss: 1.76387\n",
      "Epoch: 0 | Iteration: 117 | Classification loss: 0.85201 | Regression loss: 1.03881 | Running loss: 1.76494\n",
      "Epoch: 0 | Iteration: 118 | Classification loss: 0.93374 | Regression loss: 0.88116 | Running loss: 1.76536\n",
      "Epoch: 0 | Iteration: 119 | Classification loss: 0.86833 | Regression loss: 0.88852 | Running loss: 1.76529\n",
      "Epoch: 0 | Iteration: 120 | Classification loss: 0.91301 | Regression loss: 0.99117 | Running loss: 1.76644\n",
      "Epoch: 0 | Iteration: 121 | Classification loss: 0.82699 | Regression loss: 0.91632 | Running loss: 1.76625\n",
      "Epoch: 0 | Iteration: 122 | Classification loss: 0.57047 | Regression loss: 0.50823 | Running loss: 1.76066\n",
      "Epoch: 0 | Iteration: 123 | Classification loss: 0.62069 | Regression loss: 0.44072 | Running loss: 1.75502\n",
      "Epoch: 0 | Iteration: 124 | Classification loss: 0.74633 | Regression loss: 1.07522 | Running loss: 1.75555\n",
      "Epoch: 0 | Iteration: 125 | Classification loss: 0.82123 | Regression loss: 0.81382 | Running loss: 1.75460\n",
      "Epoch: 0 | Iteration: 126 | Classification loss: 0.89964 | Regression loss: 0.79983 | Running loss: 1.75416\n",
      "Epoch: 0 | Iteration: 127 | Classification loss: 0.89806 | Regression loss: 0.86685 | Running loss: 1.75425\n",
      "Epoch: 0 | Iteration: 128 | Classification loss: 0.90017 | Regression loss: 0.69924 | Running loss: 1.75305\n",
      "Epoch: 0 | Iteration: 129 | Classification loss: 0.77990 | Regression loss: 1.09395 | Running loss: 1.75398\n",
      "Epoch: 0 | Iteration: 130 | Classification loss: 0.42595 | Regression loss: 0.41179 | Running loss: 1.74698\n",
      "Epoch: 0 | Iteration: 131 | Classification loss: 0.69228 | Regression loss: 0.94903 | Running loss: 1.74618\n",
      "Epoch: 0 | Iteration: 132 | Classification loss: 0.74553 | Regression loss: 0.29966 | Running loss: 1.74091\n",
      "Epoch: 0 | Iteration: 133 | Classification loss: 0.85856 | Regression loss: 0.97978 | Running loss: 1.74164\n",
      "Epoch: 0 | Iteration: 134 | Classification loss: 0.98778 | Regression loss: 0.75108 | Running loss: 1.74162\n",
      "Epoch: 0 | Iteration: 135 | Classification loss: 0.74783 | Regression loss: 0.86033 | Running loss: 1.74064\n",
      "Epoch: 0 | Iteration: 136 | Classification loss: 0.80305 | Regression loss: 0.84958 | Running loss: 1.73999\n",
      "Epoch: 0 | Iteration: 137 | Classification loss: 0.88395 | Regression loss: 0.83057 | Running loss: 1.73981\n",
      "Epoch: 0 | Iteration: 138 | Classification loss: 0.70821 | Regression loss: 0.98760 | Running loss: 1.73949\n",
      "Epoch: 0 | Iteration: 139 | Classification loss: 0.93811 | Regression loss: 0.83163 | Running loss: 1.73971\n",
      "Epoch: 0 | Iteration: 140 | Classification loss: 0.55377 | Regression loss: 0.44596 | Running loss: 1.73446\n",
      "Epoch: 0 | Iteration: 141 | Classification loss: 0.54677 | Regression loss: 0.37669 | Running loss: 1.72875\n",
      "Epoch: 0 | Iteration: 142 | Classification loss: 0.72582 | Regression loss: 0.47645 | Running loss: 1.72507\n",
      "Epoch: 0 | Iteration: 143 | Classification loss: 0.89674 | Regression loss: 0.92134 | Running loss: 1.72571\n",
      "Epoch: 0 | Iteration: 144 | Classification loss: 0.86145 | Regression loss: 0.77616 | Running loss: 1.72511\n",
      "Epoch: 0 | Iteration: 145 | Classification loss: 0.85244 | Regression loss: 0.78076 | Running loss: 1.72448\n",
      "Epoch: 0 | Iteration: 146 | Classification loss: 0.70712 | Regression loss: 0.90880 | Running loss: 1.72374\n",
      "Epoch: 0 | Iteration: 147 | Classification loss: 0.91857 | Regression loss: 0.86371 | Running loss: 1.72413\n",
      "Epoch: 0 | Iteration: 148 | Classification loss: 1.12436 | Regression loss: 0.84354 | Running loss: 1.72577\n",
      "Epoch: 0 | Iteration: 149 | Classification loss: 0.74300 | Regression loss: 0.48051 | Running loss: 1.72242\n",
      "Epoch: 0 | Iteration: 150 | Classification loss: 0.76473 | Regression loss: 0.44685 | Running loss: 1.71904\n",
      "Epoch: 0 | Iteration: 151 | Classification loss: 0.84134 | Regression loss: 0.82422 | Running loss: 1.71869\n",
      "Epoch: 0 | Iteration: 152 | Classification loss: 0.84258 | Regression loss: 0.99187 | Running loss: 1.71944\n",
      "Epoch: 0 | Iteration: 153 | Classification loss: 0.73768 | Regression loss: 0.38375 | Running loss: 1.71556\n",
      "Epoch: 0 | Iteration: 154 | Classification loss: 0.83420 | Regression loss: 0.93942 | Running loss: 1.71593\n",
      "Epoch: 0 | Iteration: 155 | Classification loss: 0.86994 | Regression loss: 0.93707 | Running loss: 1.71652\n",
      "Epoch: 0 | Iteration: 156 | Classification loss: 0.92973 | Regression loss: 0.87528 | Running loss: 1.71708\n",
      "Epoch: 0 | Iteration: 157 | Classification loss: 0.99323 | Regression loss: 0.79910 | Running loss: 1.71756\n",
      "Epoch: 0 | Iteration: 158 | Classification loss: 0.90752 | Regression loss: 0.79851 | Running loss: 1.71749\n",
      "Epoch: 0 | Iteration: 159 | Classification loss: 0.75137 | Regression loss: 0.67856 | Running loss: 1.71569\n",
      "Epoch: 0 | Iteration: 160 | Classification loss: 1.11402 | Regression loss: 1.02339 | Running loss: 1.71831\n",
      "Epoch: 0 | Iteration: 161 | Classification loss: 0.91757 | Regression loss: 0.63554 | Running loss: 1.71729\n",
      "Epoch: 0 | Iteration: 162 | Classification loss: 0.67591 | Regression loss: 0.90311 | Running loss: 1.71644\n",
      "Epoch: 0 | Iteration: 163 | Classification loss: 1.03425 | Regression loss: 0.43379 | Running loss: 1.71493\n",
      "Epoch: 0 | Iteration: 164 | Classification loss: 0.64294 | Regression loss: 0.97673 | Running loss: 1.71435\n",
      "Epoch: 0 | Iteration: 165 | Classification loss: 0.92027 | Regression loss: 0.97132 | Running loss: 1.71542\n",
      "Epoch: 0 | Iteration: 166 | Classification loss: 0.73847 | Regression loss: 0.93932 | Running loss: 1.71519\n",
      "Epoch: 0 | Iteration: 167 | Classification loss: 0.63866 | Regression loss: 0.96006 | Running loss: 1.71450\n",
      "Epoch: 0 | Iteration: 168 | Classification loss: 0.66927 | Regression loss: 0.88065 | Running loss: 1.71352\n",
      "Epoch: 0 | Iteration: 169 | Classification loss: 0.76576 | Regression loss: 0.76290 | Running loss: 1.71244\n",
      "Epoch: 0 | Iteration: 170 | Classification loss: 0.85002 | Regression loss: 0.65407 | Running loss: 1.71122\n",
      "Epoch: 0 | Iteration: 171 | Classification loss: 0.95430 | Regression loss: 0.97770 | Running loss: 1.71250\n",
      "Epoch: 0 | Iteration: 172 | Classification loss: 0.67431 | Regression loss: 0.98076 | Running loss: 1.71217\n",
      "Epoch: 0 | Iteration: 173 | Classification loss: 0.67015 | Regression loss: 0.82637 | Running loss: 1.71093\n",
      "Epoch: 0 | Iteration: 174 | Classification loss: 0.71135 | Regression loss: 0.97960 | Running loss: 1.71082\n",
      "Epoch: 0 | Iteration: 175 | Classification loss: 0.82588 | Regression loss: 0.46312 | Running loss: 1.70842\n",
      "Epoch: 0 | Iteration: 176 | Classification loss: 0.84588 | Regression loss: 0.85651 | Running loss: 1.70838\n",
      "Epoch: 0 | Iteration: 177 | Classification loss: 1.18465 | Regression loss: 0.93835 | Running loss: 1.71071\n",
      "Epoch: 0 | Iteration: 178 | Classification loss: 0.62607 | Regression loss: 0.45525 | Running loss: 1.70720\n",
      "Epoch: 0 | Iteration: 179 | Classification loss: 0.71966 | Regression loss: 0.80772 | Running loss: 1.70620\n",
      "Epoch: 0 | Iteration: 180 | Classification loss: 0.91013 | Regression loss: 0.90111 | Running loss: 1.70678\n",
      "Epoch: 0 | Iteration: 181 | Classification loss: 0.94940 | Regression loss: 0.91775 | Running loss: 1.70766\n",
      "Epoch: 0 | Iteration: 182 | Classification loss: 0.54756 | Regression loss: 0.38212 | Running loss: 1.70341\n",
      "Epoch: 0 | Iteration: 183 | Classification loss: 0.98573 | Regression loss: 0.96237 | Running loss: 1.70474\n",
      "Epoch: 0 | Iteration: 184 | Classification loss: 0.73858 | Regression loss: 0.92082 | Running loss: 1.70449\n",
      "Epoch: 0 | Iteration: 185 | Classification loss: 0.86188 | Regression loss: 0.86706 | Running loss: 1.70463\n",
      "Epoch: 0 | Iteration: 186 | Classification loss: 0.88420 | Regression loss: 1.00711 | Running loss: 1.70562\n",
      "Epoch: 0 | Iteration: 187 | Classification loss: 0.23501 | Regression loss: 0.00000 | Running loss: 1.69780\n",
      "Epoch: 0 | Iteration: 188 | Classification loss: 0.82585 | Regression loss: 0.87720 | Running loss: 1.69783\n",
      "Epoch: 0 | Iteration: 189 | Classification loss: 0.88591 | Regression loss: 1.04538 | Running loss: 1.69906\n",
      "Epoch: 0 | Iteration: 190 | Classification loss: 0.62999 | Regression loss: 0.87534 | Running loss: 1.69804\n",
      "Epoch: 0 | Iteration: 191 | Classification loss: 0.62060 | Regression loss: 0.90342 | Running loss: 1.69714\n",
      "Epoch: 0 | Iteration: 192 | Classification loss: 0.82932 | Regression loss: 0.94120 | Running loss: 1.69752\n",
      "Epoch: 0 | Iteration: 193 | Classification loss: 0.94496 | Regression loss: 1.09806 | Running loss: 1.69930\n",
      "Epoch: 0 | Iteration: 194 | Classification loss: 0.69905 | Regression loss: 0.87884 | Running loss: 1.69868\n",
      "Epoch: 0 | Iteration: 195 | Classification loss: 0.67968 | Regression loss: 1.20907 | Running loss: 1.69965\n",
      "Epoch: 0 | Iteration: 196 | Classification loss: 1.12069 | Regression loss: 0.48752 | Running loss: 1.69918\n",
      "Epoch: 0 | Iteration: 197 | Classification loss: 0.59370 | Regression loss: 0.49655 | Running loss: 1.69611\n",
      "Epoch: 0 | Iteration: 198 | Classification loss: 0.88528 | Regression loss: 1.00965 | Running loss: 1.69711\n",
      "Epoch: 0 | Iteration: 199 | Classification loss: 0.75950 | Regression loss: 0.99544 | Running loss: 1.69739\n",
      "Epoch: 0 | Iteration: 200 | Classification loss: 0.74024 | Regression loss: 0.88392 | Running loss: 1.69703\n",
      "Epoch: 0 | Iteration: 201 | Classification loss: 0.66037 | Regression loss: 0.40614 | Running loss: 1.69391\n",
      "Epoch: 0 | Iteration: 202 | Classification loss: 0.78374 | Regression loss: 0.89003 | Running loss: 1.69381\n",
      "Epoch: 0 | Iteration: 203 | Classification loss: 0.88604 | Regression loss: 0.92165 | Running loss: 1.69437\n",
      "Epoch: 0 | Iteration: 204 | Classification loss: 0.85344 | Regression loss: 0.72279 | Running loss: 1.69379\n",
      "Epoch: 0 | Iteration: 205 | Classification loss: 0.85970 | Regression loss: 0.88941 | Running loss: 1.69406\n",
      "Epoch: 0 | Iteration: 206 | Classification loss: 0.80805 | Regression loss: 1.02192 | Running loss: 1.69472\n",
      "Epoch: 0 | Iteration: 207 | Classification loss: 0.70920 | Regression loss: 0.94200 | Running loss: 1.69451\n",
      "Epoch: 0 | Iteration: 208 | Classification loss: 0.65884 | Regression loss: 0.95338 | Running loss: 1.69411\n",
      "Epoch: 0 | Iteration: 209 | Classification loss: 0.97701 | Regression loss: 0.38558 | Running loss: 1.69253\n",
      "Epoch: 0 | Iteration: 210 | Classification loss: 0.72361 | Regression loss: 0.93316 | Running loss: 1.69237\n",
      "Epoch: 0 | Iteration: 211 | Classification loss: 0.94985 | Regression loss: 0.85899 | Running loss: 1.69291\n",
      "Epoch: 0 | Iteration: 212 | Classification loss: 0.71235 | Regression loss: 0.88618 | Running loss: 1.69247\n",
      "Epoch: 0 | Iteration: 213 | Classification loss: 0.62982 | Regression loss: 0.93979 | Running loss: 1.69190\n",
      "Epoch: 0 | Iteration: 214 | Classification loss: 0.81794 | Regression loss: 0.78368 | Running loss: 1.69148\n",
      "Epoch: 0 | Iteration: 215 | Classification loss: 0.83072 | Regression loss: 1.05401 | Running loss: 1.69237\n",
      "Epoch: 0 | Iteration: 216 | Classification loss: 0.75877 | Regression loss: 0.97374 | Running loss: 1.69256\n",
      "Epoch: 0 | Iteration: 217 | Classification loss: 0.74837 | Regression loss: 0.94913 | Running loss: 1.69258\n",
      "Epoch: 0 | Iteration: 218 | Classification loss: 0.78469 | Regression loss: 0.83955 | Running loss: 1.69227\n",
      "Epoch: 0 | Iteration: 219 | Classification loss: 1.55113 | Regression loss: 0.38661 | Running loss: 1.69338\n",
      "Epoch: 0 | Iteration: 220 | Classification loss: 0.80871 | Regression loss: 0.76298 | Running loss: 1.69283\n",
      "Epoch: 0 | Iteration: 221 | Classification loss: 0.80996 | Regression loss: 0.93310 | Running loss: 1.69306\n",
      "Epoch: 0 | Iteration: 222 | Classification loss: 0.95297 | Regression loss: 0.81955 | Running loss: 1.69342\n",
      "Epoch: 0 | Iteration: 223 | Classification loss: 0.81178 | Regression loss: 0.94855 | Running loss: 1.69371\n",
      "Epoch: 0 | Iteration: 224 | Classification loss: 0.86589 | Regression loss: 0.79453 | Running loss: 1.69357\n",
      "Epoch: 0 | Iteration: 225 | Classification loss: 0.92246 | Regression loss: 1.00869 | Running loss: 1.69462\n",
      "Epoch: 0 | Iteration: 226 | Classification loss: 0.93102 | Regression loss: 0.93453 | Running loss: 1.69537\n",
      "Epoch: 0 | Iteration: 227 | Classification loss: 0.81379 | Regression loss: 0.81597 | Running loss: 1.69508\n",
      "Epoch: 0 | Iteration: 228 | Classification loss: 0.75326 | Regression loss: 0.79069 | Running loss: 1.69442\n",
      "Epoch: 0 | Iteration: 229 | Classification loss: 0.92472 | Regression loss: 0.91676 | Running loss: 1.69506\n",
      "Epoch: 0 | Iteration: 230 | Classification loss: 0.77481 | Regression loss: 1.01052 | Running loss: 1.69545\n",
      "Epoch: 0 | Iteration: 231 | Classification loss: 0.88182 | Regression loss: 0.81772 | Running loss: 1.69547\n",
      "Epoch: 0 | Iteration: 232 | Classification loss: 0.84210 | Regression loss: 0.91506 | Running loss: 1.69574\n",
      "Epoch: 0 | Iteration: 233 | Classification loss: 0.67842 | Regression loss: 0.86996 | Running loss: 1.69511\n",
      "Epoch: 0 | Iteration: 234 | Classification loss: 0.65773 | Regression loss: 0.92750 | Running loss: 1.69464\n",
      "Epoch: 0 | Iteration: 235 | Classification loss: 0.91332 | Regression loss: 0.75886 | Running loss: 1.69454\n",
      "Epoch: 0 | Iteration: 236 | Classification loss: 2.65619 | Regression loss: 0.00000 | Running loss: 1.69860\n",
      "Epoch: 0 | Iteration: 237 | Classification loss: 0.70559 | Regression loss: 0.73499 | Running loss: 1.69752\n",
      "Epoch: 0 | Iteration: 238 | Classification loss: 0.86634 | Regression loss: 0.28822 | Running loss: 1.69524\n",
      "Epoch: 0 | Iteration: 239 | Classification loss: 1.18186 | Regression loss: 0.39147 | Running loss: 1.69474\n",
      "Epoch: 0 | Iteration: 240 | Classification loss: 0.79832 | Regression loss: 1.05117 | Running loss: 1.69538\n",
      "Epoch: 0 | Iteration: 241 | Classification loss: 0.79908 | Regression loss: 0.86738 | Running loss: 1.69526\n",
      "Epoch: 0 | Iteration: 242 | Classification loss: 0.82250 | Regression loss: 0.84193 | Running loss: 1.69513\n",
      "Epoch: 0 | Iteration: 243 | Classification loss: 0.80825 | Regression loss: 0.95460 | Running loss: 1.69541\n",
      "Epoch: 0 | Iteration: 244 | Classification loss: 0.92071 | Regression loss: 0.94611 | Running loss: 1.69611\n",
      "Epoch: 0 | Iteration: 245 | Classification loss: 1.02932 | Regression loss: 0.63908 | Running loss: 1.69600\n",
      "Epoch: 0 | Iteration: 246 | Classification loss: 0.62950 | Regression loss: 0.53493 | Running loss: 1.69384\n",
      "Epoch: 0 | Iteration: 247 | Classification loss: 0.97203 | Regression loss: 1.10256 | Running loss: 1.69538\n",
      "Epoch: 0 | Iteration: 248 | Classification loss: 0.82870 | Regression loss: 0.96120 | Running loss: 1.69576\n",
      "Epoch: 0 | Iteration: 249 | Classification loss: 0.78364 | Regression loss: 0.92461 | Running loss: 1.69581\n",
      "Epoch: 0 | Iteration: 250 | Classification loss: 0.53034 | Regression loss: 0.51783 | Running loss: 1.69323\n",
      "Epoch: 0 | Iteration: 251 | Classification loss: 1.07935 | Regression loss: 0.89128 | Running loss: 1.69433\n",
      "Epoch: 0 | Iteration: 252 | Classification loss: 0.60851 | Regression loss: 0.47057 | Running loss: 1.69190\n",
      "Epoch: 0 | Iteration: 253 | Classification loss: 0.78918 | Regression loss: 0.84770 | Running loss: 1.69168\n",
      "Epoch: 0 | Iteration: 254 | Classification loss: 0.80037 | Regression loss: 0.90106 | Running loss: 1.69172\n",
      "Epoch: 0 | Iteration: 255 | Classification loss: 0.62608 | Regression loss: 0.46346 | Running loss: 1.68937\n",
      "Epoch: 0 | Iteration: 256 | Classification loss: 0.84097 | Regression loss: 0.94241 | Running loss: 1.68973\n",
      "Epoch: 0 | Iteration: 257 | Classification loss: 0.68956 | Regression loss: 0.77175 | Running loss: 1.68885\n",
      "Epoch: 0 | Iteration: 258 | Classification loss: 1.00698 | Regression loss: 0.77784 | Running loss: 1.68922\n",
      "Epoch: 0 | Iteration: 259 | Classification loss: 0.77180 | Regression loss: 0.93167 | Running loss: 1.68927\n",
      "Epoch: 0 | Iteration: 260 | Classification loss: 0.96054 | Regression loss: 0.93151 | Running loss: 1.69005\n",
      "Epoch: 0 | Iteration: 261 | Classification loss: 0.66088 | Regression loss: 0.98767 | Running loss: 1.68989\n",
      "Epoch: 0 | Iteration: 262 | Classification loss: 0.83092 | Regression loss: 0.79078 | Running loss: 1.68963\n",
      "Epoch: 0 | Iteration: 263 | Classification loss: 0.69963 | Regression loss: 1.00211 | Running loss: 1.68968\n",
      "Epoch: 0 | Iteration: 264 | Classification loss: 0.94409 | Regression loss: 0.37084 | Running loss: 1.68826\n",
      "Epoch: 0 | Iteration: 265 | Classification loss: 0.84946 | Regression loss: 0.95791 | Running loss: 1.68871\n",
      "Epoch: 0 | Iteration: 266 | Classification loss: 0.76869 | Regression loss: 0.96435 | Running loss: 1.68888\n",
      "Epoch: 0 | Iteration: 267 | Classification loss: 0.76508 | Regression loss: 0.48257 | Running loss: 1.68723\n",
      "Epoch: 0 | Iteration: 268 | Classification loss: 0.91814 | Regression loss: 0.88765 | Running loss: 1.68767\n",
      "Epoch: 0 | Iteration: 269 | Classification loss: 0.81504 | Regression loss: 0.65930 | Running loss: 1.68688\n",
      "Epoch: 0 | Iteration: 270 | Classification loss: 0.75673 | Regression loss: 0.90844 | Running loss: 1.68680\n",
      "Epoch: 0 | Iteration: 271 | Classification loss: 0.71824 | Regression loss: 1.15368 | Running loss: 1.68748\n",
      "Epoch: 0 | Iteration: 272 | Classification loss: 0.74441 | Regression loss: 1.00054 | Running loss: 1.68769\n",
      "Epoch: 0 | Iteration: 273 | Classification loss: 0.83969 | Regression loss: 0.94806 | Running loss: 1.68806\n",
      "Epoch: 0 | Iteration: 274 | Classification loss: 0.78002 | Regression loss: 0.92447 | Running loss: 1.68812\n",
      "Epoch: 0 | Iteration: 275 | Classification loss: 0.66806 | Regression loss: 0.95437 | Running loss: 1.68788\n",
      "Epoch: 0 | Iteration: 276 | Classification loss: 0.76623 | Regression loss: 0.80866 | Running loss: 1.68747\n",
      "Epoch: 0 | Iteration: 277 | Classification loss: 1.03318 | Regression loss: 0.47717 | Running loss: 1.68684\n",
      "Epoch: 0 | Iteration: 278 | Classification loss: 0.90160 | Regression loss: 0.91183 | Running loss: 1.68729\n",
      "Epoch: 0 | Iteration: 279 | Classification loss: 1.21073 | Regression loss: 0.84765 | Running loss: 1.68861\n",
      "Epoch: 0 | Iteration: 280 | Classification loss: 0.70145 | Regression loss: 0.88520 | Running loss: 1.68825\n",
      "Epoch: 0 | Iteration: 281 | Classification loss: 0.65507 | Regression loss: 0.96449 | Running loss: 1.68801\n",
      "Epoch: 0 | Iteration: 282 | Classification loss: 0.69529 | Regression loss: 0.82107 | Running loss: 1.68740\n",
      "Epoch: 0 | Iteration: 283 | Classification loss: 0.92707 | Regression loss: 0.92215 | Running loss: 1.68797\n",
      "Epoch: 0 | Iteration: 284 | Classification loss: 0.73062 | Regression loss: 0.90973 | Running loss: 1.68780\n",
      "Epoch: 0 | Iteration: 285 | Classification loss: 0.75903 | Regression loss: 0.86648 | Running loss: 1.68759\n",
      "Epoch: 0 | Iteration: 286 | Classification loss: 0.99492 | Regression loss: 0.26629 | Running loss: 1.68610\n",
      "Epoch: 0 | Iteration: 287 | Classification loss: 0.87344 | Regression loss: 0.93297 | Running loss: 1.68652\n",
      "Epoch: 0 | Iteration: 288 | Classification loss: 0.62530 | Regression loss: 0.51396 | Running loss: 1.68462\n",
      "Epoch: 0 | Iteration: 289 | Classification loss: 0.80400 | Regression loss: 0.99880 | Running loss: 1.68503\n",
      "Epoch: 0 | Iteration: 290 | Classification loss: 0.76113 | Regression loss: 0.82315 | Running loss: 1.68469\n",
      "Epoch: 0 | Iteration: 291 | Classification loss: 0.79547 | Regression loss: 0.93484 | Running loss: 1.68484\n",
      "Epoch: 0 | Iteration: 292 | Classification loss: 0.82084 | Regression loss: 0.68703 | Running loss: 1.68424\n",
      "Epoch: 0 | Iteration: 293 | Classification loss: 0.60949 | Regression loss: 0.47466 | Running loss: 1.68220\n",
      "Epoch: 0 | Iteration: 294 | Classification loss: 0.89953 | Regression loss: 1.01587 | Running loss: 1.68299\n",
      "Epoch: 0 | Iteration: 295 | Classification loss: 0.90404 | Regression loss: 0.99512 | Running loss: 1.68372\n",
      "Epoch: 0 | Iteration: 296 | Classification loss: 0.51617 | Regression loss: 0.36329 | Running loss: 1.68101\n",
      "Epoch: 0 | Iteration: 297 | Classification loss: 0.87586 | Regression loss: 0.85332 | Running loss: 1.68117\n",
      "Epoch: 0 | Iteration: 298 | Classification loss: 0.83623 | Regression loss: 1.05214 | Running loss: 1.68186\n",
      "Epoch: 0 | Iteration: 299 | Classification loss: 0.57082 | Regression loss: 0.46113 | Running loss: 1.67970\n",
      "Epoch: 0 | Iteration: 300 | Classification loss: 0.77838 | Regression loss: 0.93118 | Running loss: 1.67980\n",
      "Epoch: 0 | Iteration: 301 | Classification loss: 0.79865 | Regression loss: 0.58390 | Running loss: 1.67881\n",
      "Epoch: 0 | Iteration: 302 | Classification loss: 0.76751 | Regression loss: 0.81654 | Running loss: 1.67850\n",
      "Epoch: 0 | Iteration: 303 | Classification loss: 0.66454 | Regression loss: 0.92334 | Running loss: 1.67820\n",
      "Epoch: 0 | Iteration: 304 | Classification loss: 0.59813 | Regression loss: 0.85224 | Running loss: 1.67746\n",
      "Epoch: 0 | Iteration: 305 | Classification loss: 0.72200 | Regression loss: 1.19854 | Running loss: 1.67825\n",
      "Epoch: 0 | Iteration: 306 | Classification loss: 0.75497 | Regression loss: 1.00073 | Running loss: 1.67850\n",
      "Epoch: 0 | Iteration: 307 | Classification loss: 0.74075 | Regression loss: 0.82607 | Running loss: 1.67814\n",
      "Epoch: 0 | Iteration: 308 | Classification loss: 0.61510 | Regression loss: 0.81916 | Running loss: 1.67735\n",
      "Epoch: 0 | Iteration: 309 | Classification loss: 0.73447 | Regression loss: 0.86046 | Running loss: 1.67708\n",
      "Epoch: 0 | Iteration: 310 | Classification loss: 0.59230 | Regression loss: 0.91250 | Running loss: 1.67653\n",
      "Epoch: 0 | Iteration: 311 | Classification loss: 0.63473 | Regression loss: 1.02257 | Running loss: 1.67647\n",
      "Epoch: 0 | Iteration: 312 | Classification loss: 1.25761 | Regression loss: 0.39486 | Running loss: 1.67639\n",
      "Epoch: 0 | Iteration: 313 | Classification loss: 0.56882 | Regression loss: 1.02216 | Running loss: 1.67612\n",
      "Epoch: 0 | Iteration: 314 | Classification loss: 0.65860 | Regression loss: 0.91549 | Running loss: 1.67580\n",
      "Epoch: 0 | Iteration: 315 | Classification loss: 0.67249 | Regression loss: 0.85946 | Running loss: 1.67534\n",
      "Epoch: 0 | Iteration: 316 | Classification loss: 0.71860 | Regression loss: 1.04587 | Running loss: 1.67562\n",
      "Epoch: 0 | Iteration: 317 | Classification loss: 0.61525 | Regression loss: 0.88980 | Running loss: 1.67509\n",
      "Epoch: 0 | Iteration: 318 | Classification loss: 0.68058 | Regression loss: 0.99262 | Running loss: 1.67508\n",
      "Epoch: 0 | Iteration: 319 | Classification loss: 0.76018 | Regression loss: 0.92913 | Running loss: 1.67512\n",
      "Epoch: 0 | Iteration: 320 | Classification loss: 0.82236 | Regression loss: 0.93419 | Running loss: 1.67538\n",
      "Epoch: 0 | Iteration: 321 | Classification loss: 0.87519 | Regression loss: 0.92599 | Running loss: 1.67577\n",
      "Epoch: 0 | Iteration: 322 | Classification loss: 0.54004 | Regression loss: 0.42331 | Running loss: 1.67356\n",
      "Epoch: 0 | Iteration: 323 | Classification loss: 0.80292 | Regression loss: 0.97639 | Running loss: 1.67389\n",
      "Epoch: 0 | Iteration: 324 | Classification loss: 0.81719 | Regression loss: 0.83034 | Running loss: 1.67381\n",
      "Epoch: 0 | Iteration: 325 | Classification loss: 0.59078 | Regression loss: 0.99394 | Running loss: 1.67353\n",
      "Epoch: 0 | Iteration: 326 | Classification loss: 0.81481 | Regression loss: 1.03137 | Running loss: 1.67406\n",
      "Epoch: 0 | Iteration: 327 | Classification loss: 0.67705 | Regression loss: 0.85471 | Running loss: 1.67363\n",
      "Epoch: 0 | Iteration: 328 | Classification loss: 0.83488 | Regression loss: 0.91823 | Running loss: 1.67387\n",
      "Epoch: 0 | Iteration: 329 | Classification loss: 0.75935 | Regression loss: 0.90154 | Running loss: 1.67383\n",
      "Epoch: 0 | Iteration: 330 | Classification loss: 0.65572 | Regression loss: 0.91825 | Running loss: 1.67353\n",
      "Epoch: 0 | Iteration: 331 | Classification loss: 0.87609 | Regression loss: 0.83470 | Running loss: 1.67364\n",
      "Epoch: 0 | Iteration: 332 | Classification loss: 0.83052 | Regression loss: 0.94295 | Running loss: 1.67394\n",
      "Epoch: 0 | Iteration: 333 | Classification loss: 0.63590 | Regression loss: 0.66781 | Running loss: 1.67283\n",
      "Epoch: 0 | Iteration: 334 | Classification loss: 0.38675 | Regression loss: 0.00000 | Running loss: 1.66899\n",
      "Epoch: 0 | Iteration: 335 | Classification loss: 0.70951 | Regression loss: 0.92126 | Running loss: 1.66888\n",
      "Epoch: 0 | Iteration: 336 | Classification loss: 0.66700 | Regression loss: 0.80015 | Running loss: 1.66828\n",
      "Epoch: 0 | Iteration: 337 | Classification loss: 0.86421 | Regression loss: 0.90689 | Running loss: 1.66859\n",
      "Epoch: 0 | Iteration: 338 | Classification loss: 0.87507 | Regression loss: 0.91725 | Running loss: 1.66895\n",
      "Epoch: 0 | Iteration: 339 | Classification loss: 0.72164 | Regression loss: 1.04732 | Running loss: 1.66924\n",
      "Epoch: 0 | Iteration: 340 | Classification loss: 0.75210 | Regression loss: 0.88600 | Running loss: 1.66915\n",
      "Epoch: 0 | Iteration: 341 | Classification loss: 0.64162 | Regression loss: 0.80209 | Running loss: 1.66849\n",
      "Epoch: 0 | Iteration: 342 | Classification loss: 0.73880 | Regression loss: 0.94024 | Running loss: 1.66853\n",
      "Epoch: 0 | Iteration: 343 | Classification loss: 0.75120 | Regression loss: 0.96942 | Running loss: 1.66868\n",
      "Epoch: 0 | Iteration: 344 | Classification loss: 0.78805 | Regression loss: 1.01561 | Running loss: 1.66907\n",
      "Epoch: 0 | Iteration: 345 | Classification loss: 0.50403 | Regression loss: 0.84298 | Running loss: 1.66814\n",
      "Epoch: 0 | Iteration: 346 | Classification loss: 0.87215 | Regression loss: 0.83162 | Running loss: 1.66824\n",
      "Epoch: 0 | Iteration: 347 | Classification loss: 2.08945 | Regression loss: 0.97211 | Running loss: 1.67224\n",
      "Epoch: 0 | Iteration: 348 | Classification loss: 0.78694 | Regression loss: 0.85967 | Running loss: 1.67217\n",
      "Epoch: 0 | Iteration: 349 | Classification loss: 0.47359 | Regression loss: 0.42706 | Running loss: 1.66997\n",
      "Epoch: 0 | Iteration: 350 | Classification loss: 0.72932 | Regression loss: 1.07482 | Running loss: 1.67035\n",
      "Epoch: 0 | Iteration: 351 | Classification loss: 0.63301 | Regression loss: 1.04057 | Running loss: 1.67036\n",
      "Epoch: 0 | Iteration: 352 | Classification loss: 0.58152 | Regression loss: 0.46337 | Running loss: 1.66859\n",
      "Epoch: 0 | Iteration: 353 | Classification loss: 0.81678 | Regression loss: 0.95336 | Running loss: 1.66887\n",
      "Epoch: 0 | Iteration: 354 | Classification loss: 0.89267 | Regression loss: 0.78474 | Running loss: 1.66890\n",
      "Epoch: 0 | Iteration: 355 | Classification loss: 0.97296 | Regression loss: 0.84397 | Running loss: 1.66931\n",
      "Epoch: 0 | Iteration: 356 | Classification loss: 0.47728 | Regression loss: 0.52330 | Running loss: 1.66744\n",
      "Epoch: 0 | Iteration: 357 | Classification loss: 0.92149 | Regression loss: 1.03310 | Running loss: 1.66824\n",
      "Epoch: 0 | Iteration: 358 | Classification loss: 0.87002 | Regression loss: 1.01110 | Running loss: 1.66883\n",
      "Epoch: 0 | Iteration: 359 | Classification loss: 0.92316 | Regression loss: 0.85024 | Running loss: 1.66912\n",
      "Epoch: 0 | Iteration: 360 | Classification loss: 0.84924 | Regression loss: 0.87183 | Running loss: 1.66927\n",
      "Epoch: 0 | Iteration: 361 | Classification loss: 0.86990 | Regression loss: 1.00922 | Running loss: 1.66985\n",
      "Epoch: 0 | Iteration: 362 | Classification loss: 0.69597 | Regression loss: 0.86485 | Running loss: 1.66955\n",
      "Epoch: 0 | Iteration: 363 | Classification loss: 0.64584 | Regression loss: 0.45291 | Running loss: 1.66798\n",
      "Epoch: 0 | Iteration: 364 | Classification loss: 0.82760 | Regression loss: 0.93832 | Running loss: 1.66825\n",
      "Epoch: 0 | Iteration: 365 | Classification loss: 0.75500 | Regression loss: 0.91682 | Running loss: 1.66826\n",
      "Epoch: 0 | Iteration: 366 | Classification loss: 1.47414 | Regression loss: 0.50146 | Running loss: 1.66910\n",
      "Epoch: 0 | Iteration: 367 | Classification loss: 0.72095 | Regression loss: 0.00000 | Running loss: 1.66652\n",
      "Epoch: 0 | Iteration: 368 | Classification loss: 1.09446 | Regression loss: 0.92852 | Running loss: 1.66748\n",
      "Epoch: 0 | Iteration: 369 | Classification loss: 1.49937 | Regression loss: 0.81027 | Running loss: 1.66922\n",
      "Epoch: 0 | Iteration: 370 | Classification loss: 0.52836 | Regression loss: 0.75546 | Running loss: 1.66818\n",
      "Epoch: 0 | Iteration: 371 | Classification loss: 0.87361 | Regression loss: 0.86213 | Running loss: 1.66836\n",
      "Epoch: 0 | Iteration: 372 | Classification loss: 0.19541 | Regression loss: 0.00000 | Running loss: 1.66441\n",
      "Epoch: 0 | Iteration: 373 | Classification loss: 0.88266 | Regression loss: 0.66718 | Running loss: 1.66411\n",
      "Epoch: 0 | Iteration: 374 | Classification loss: 0.66039 | Regression loss: 0.71631 | Running loss: 1.66334\n",
      "Epoch: 0 | Iteration: 375 | Classification loss: 0.75731 | Regression loss: 0.88835 | Running loss: 1.66329\n",
      "Epoch: 0 | Iteration: 376 | Classification loss: 0.85859 | Regression loss: 0.74468 | Running loss: 1.66314\n",
      "Epoch: 0 | Iteration: 377 | Classification loss: 0.88649 | Regression loss: 0.88401 | Running loss: 1.66342\n",
      "Epoch: 0 | Iteration: 378 | Classification loss: 0.77003 | Regression loss: 0.79738 | Running loss: 1.66317\n",
      "Epoch: 0 | Iteration: 379 | Classification loss: 0.82140 | Regression loss: 0.93189 | Running loss: 1.66340\n",
      "Epoch: 0 | Iteration: 380 | Classification loss: 0.48678 | Regression loss: 0.47063 | Running loss: 1.66155\n",
      "Epoch: 0 | Iteration: 381 | Classification loss: 0.87266 | Regression loss: 0.99227 | Running loss: 1.66208\n",
      "Epoch: 0 | Iteration: 382 | Classification loss: 0.87914 | Regression loss: 0.86923 | Running loss: 1.66231\n",
      "Epoch: 0 | Iteration: 383 | Classification loss: 0.62538 | Regression loss: 0.46422 | Running loss: 1.66082\n",
      "Epoch: 0 | Iteration: 384 | Classification loss: 0.75143 | Regression loss: 0.92157 | Running loss: 1.66085\n",
      "Epoch: 0 | Iteration: 385 | Classification loss: 0.85867 | Regression loss: 0.84540 | Running loss: 1.66096\n",
      "Epoch: 0 | Iteration: 386 | Classification loss: 0.82213 | Regression loss: 1.12200 | Running loss: 1.66169\n",
      "Epoch: 0 | Iteration: 387 | Classification loss: 0.66934 | Regression loss: 1.02135 | Running loss: 1.66177\n",
      "Epoch: 0 | Iteration: 388 | Classification loss: 0.83954 | Regression loss: 0.91691 | Running loss: 1.66201\n",
      "Epoch: 0 | Iteration: 389 | Classification loss: 0.80404 | Regression loss: 0.69626 | Running loss: 1.66159\n",
      "Epoch: 0 | Iteration: 390 | Classification loss: 0.67905 | Regression loss: 1.04863 | Running loss: 1.66176\n",
      "Epoch: 0 | Iteration: 391 | Classification loss: 0.65419 | Regression loss: 0.97423 | Running loss: 1.66168\n",
      "Epoch: 0 | Iteration: 392 | Classification loss: 0.79312 | Regression loss: 0.86165 | Running loss: 1.66166\n",
      "Epoch: 0 | Iteration: 393 | Classification loss: 0.67922 | Regression loss: 0.62550 | Running loss: 1.66076\n",
      "Epoch: 0 | Iteration: 394 | Classification loss: 0.66751 | Regression loss: 0.92741 | Running loss: 1.66059\n",
      "Epoch: 0 | Iteration: 395 | Classification loss: 0.62782 | Regression loss: 0.96965 | Running loss: 1.66043\n",
      "Epoch: 0 | Iteration: 396 | Classification loss: 0.75708 | Regression loss: 0.74844 | Running loss: 1.66004\n",
      "Epoch: 0 | Iteration: 397 | Classification loss: 0.55935 | Regression loss: 0.98652 | Running loss: 1.65975\n",
      "Epoch: 0 | Iteration: 398 | Classification loss: 0.77223 | Regression loss: 0.89788 | Running loss: 1.65978\n",
      "Epoch: 0 | Iteration: 399 | Classification loss: 0.73209 | Regression loss: 0.88253 | Running loss: 1.65967\n",
      "Epoch: 0 | Iteration: 400 | Classification loss: 0.57067 | Regression loss: 0.97187 | Running loss: 1.65937\n",
      "Epoch: 0 | Iteration: 401 | Classification loss: 0.93550 | Regression loss: 0.78640 | Running loss: 1.65953\n",
      "Epoch: 0 | Iteration: 402 | Classification loss: 1.58984 | Regression loss: 0.57223 | Running loss: 1.66078\n",
      "Epoch: 0 | Iteration: 403 | Classification loss: 0.50665 | Regression loss: 0.00000 | Running loss: 1.65792\n",
      "Epoch: 0 | Iteration: 404 | Classification loss: 0.85237 | Regression loss: 1.02310 | Running loss: 1.65846\n",
      "Epoch: 0 | Iteration: 405 | Classification loss: 0.91889 | Regression loss: 0.80614 | Running loss: 1.65862\n",
      "Epoch: 0 | Iteration: 406 | Classification loss: 0.78269 | Regression loss: 0.87524 | Running loss: 1.65862\n",
      "Epoch: 0 | Iteration: 407 | Classification loss: 0.72536 | Regression loss: 0.86051 | Running loss: 1.65844\n",
      "Epoch: 0 | Iteration: 408 | Classification loss: 0.46571 | Regression loss: 0.32567 | Running loss: 1.65632\n",
      "Epoch: 0 | Iteration: 409 | Classification loss: 0.73470 | Regression loss: 0.84773 | Running loss: 1.65614\n",
      "Epoch: 0 | Iteration: 410 | Classification loss: 0.38857 | Regression loss: 0.47071 | Running loss: 1.65420\n",
      "Epoch: 0 | Iteration: 411 | Classification loss: 0.66732 | Regression loss: 0.86097 | Running loss: 1.65390\n",
      "Epoch: 0 | Iteration: 412 | Classification loss: 0.62291 | Regression loss: 0.99731 | Running loss: 1.65381\n",
      "Epoch: 0 | Iteration: 413 | Classification loss: 0.76102 | Regression loss: 0.95821 | Running loss: 1.65397\n",
      "Epoch: 0 | Iteration: 414 | Classification loss: 0.83722 | Regression loss: 1.00378 | Running loss: 1.65442\n",
      "Epoch: 0 | Iteration: 415 | Classification loss: 0.44943 | Regression loss: 0.85053 | Running loss: 1.65357\n",
      "Epoch: 0 | Iteration: 416 | Classification loss: 0.85111 | Regression loss: 0.96149 | Running loss: 1.65395\n",
      "Epoch: 0 | Iteration: 417 | Classification loss: 0.77781 | Regression loss: 0.89419 | Running loss: 1.65400\n",
      "Epoch: 0 | Iteration: 418 | Classification loss: 0.69463 | Regression loss: 0.00000 | Running loss: 1.65171\n",
      "Epoch: 0 | Iteration: 419 | Classification loss: 0.48048 | Regression loss: 0.42016 | Running loss: 1.64992\n",
      "Epoch: 0 | Iteration: 420 | Classification loss: 0.72677 | Regression loss: 0.98700 | Running loss: 1.65007\n",
      "Epoch: 0 | Iteration: 421 | Classification loss: 0.77013 | Regression loss: 0.80010 | Running loss: 1.64988\n",
      "Epoch: 0 | Iteration: 422 | Classification loss: 0.76780 | Regression loss: 0.73706 | Running loss: 1.64954\n",
      "Epoch: 0 | Iteration: 423 | Classification loss: 0.56484 | Regression loss: 0.91801 | Running loss: 1.64914\n",
      "Epoch: 0 | Iteration: 424 | Classification loss: 0.67204 | Regression loss: 0.87626 | Running loss: 1.64891\n",
      "Epoch: 0 | Iteration: 425 | Classification loss: 0.97128 | Regression loss: 0.67242 | Running loss: 1.64889\n",
      "Epoch: 0 | Iteration: 426 | Classification loss: 0.69937 | Regression loss: 0.76861 | Running loss: 1.64847\n",
      "Epoch: 0 | Iteration: 427 | Classification loss: 0.61464 | Regression loss: 0.86964 | Running loss: 1.64809\n",
      "Epoch: 0 | Iteration: 428 | Classification loss: 0.71546 | Regression loss: 0.81664 | Running loss: 1.64782\n",
      "Epoch: 0 | Iteration: 429 | Classification loss: 0.91022 | Regression loss: 0.76755 | Running loss: 1.64789\n",
      "Epoch: 0 | Iteration: 430 | Classification loss: 0.84604 | Regression loss: 0.97818 | Running loss: 1.64830\n",
      "Epoch: 0 | Iteration: 431 | Classification loss: 1.06683 | Regression loss: 0.96224 | Running loss: 1.64918\n",
      "Epoch: 0 | Iteration: 432 | Classification loss: 0.56265 | Regression loss: 0.88873 | Running loss: 1.64872\n",
      "Evaluating dataset\n",
      "168/168\n",
      "mAP:\n",
      "generator.num_classes 2\n",
      "0: 0.014258276637394799\n",
      "Precision:  0.005580509051313461\n",
      "Recall:  0.5774647887323944\n",
      "1: 0.004239160791551932\n",
      "Precision:  0.005580509051313461\n",
      "Recall:  0.5774647887323944\n",
      "Epoch: 1 | Iteration: 0 | Classification loss: 0.74107 | Regression loss: 0.96103 | Running loss: 1.64884\n",
      "Epoch: 1 | Iteration: 1 | Classification loss: 0.68176 | Regression loss: 0.83579 | Running loss: 1.64854\n",
      "Epoch: 1 | Iteration: 2 | Classification loss: 2.19478 | Regression loss: 0.92918 | Running loss: 1.65193\n",
      "Epoch: 1 | Iteration: 3 | Classification loss: 0.59167 | Regression loss: 0.77349 | Running loss: 1.65127\n",
      "Epoch: 1 | Iteration: 4 | Classification loss: 0.81371 | Regression loss: 1.05593 | Running loss: 1.65177\n",
      "Epoch: 1 | Iteration: 5 | Classification loss: 0.93333 | Regression loss: 0.69135 | Running loss: 1.65171\n",
      "Epoch: 1 | Iteration: 6 | Classification loss: 1.75938 | Regression loss: 0.30587 | Running loss: 1.65265\n",
      "Epoch: 1 | Iteration: 7 | Classification loss: 0.57786 | Regression loss: 0.83926 | Running loss: 1.65211\n",
      "Epoch: 1 | Iteration: 8 | Classification loss: 0.39212 | Regression loss: 0.41431 | Running loss: 1.65020\n",
      "Epoch: 1 | Iteration: 9 | Classification loss: 0.49788 | Regression loss: 0.47272 | Running loss: 1.64866\n",
      "Epoch: 1 | Iteration: 10 | Classification loss: 0.68609 | Regression loss: 0.85416 | Running loss: 1.64842\n",
      "Epoch: 1 | Iteration: 11 | Classification loss: 0.53019 | Regression loss: 0.52256 | Running loss: 1.64708\n",
      "Epoch: 1 | Iteration: 12 | Classification loss: 0.72223 | Regression loss: 0.76759 | Running loss: 1.64673\n",
      "Epoch: 1 | Iteration: 13 | Classification loss: 0.80927 | Regression loss: 0.87546 | Running loss: 1.64681\n",
      "Epoch: 1 | Iteration: 14 | Classification loss: 0.77774 | Regression loss: 0.81739 | Running loss: 1.64670\n",
      "Epoch: 1 | Iteration: 15 | Classification loss: 0.49390 | Regression loss: 0.56669 | Running loss: 1.64539\n",
      "Epoch: 1 | Iteration: 16 | Classification loss: 0.75671 | Regression loss: 0.76587 | Running loss: 1.64512\n",
      "Epoch: 1 | Iteration: 17 | Classification loss: 0.85740 | Regression loss: 0.91011 | Running loss: 1.64539\n",
      "Epoch: 1 | Iteration: 18 | Classification loss: 0.83069 | Regression loss: 0.91393 | Running loss: 1.64561\n",
      "Epoch: 1 | Iteration: 19 | Classification loss: 0.82561 | Regression loss: 0.98324 | Running loss: 1.64597\n",
      "Epoch: 1 | Iteration: 20 | Classification loss: 0.82072 | Regression loss: 0.85890 | Running loss: 1.64605\n",
      "Epoch: 1 | Iteration: 21 | Classification loss: 0.81515 | Regression loss: 0.54413 | Running loss: 1.64542\n",
      "Epoch: 1 | Iteration: 22 | Classification loss: 0.56484 | Regression loss: 0.49600 | Running loss: 1.64413\n",
      "Epoch: 1 | Iteration: 23 | Classification loss: 0.64351 | Regression loss: 0.36959 | Running loss: 1.64275\n",
      "Epoch: 1 | Iteration: 24 | Classification loss: 0.41993 | Regression loss: 0.44785 | Running loss: 1.64106\n",
      "Epoch: 1 | Iteration: 25 | Classification loss: 0.74031 | Regression loss: 0.88707 | Running loss: 1.64103\n",
      "Epoch: 1 | Iteration: 26 | Classification loss: 0.61868 | Regression loss: 0.66706 | Running loss: 1.64026\n",
      "Epoch: 1 | Iteration: 27 | Classification loss: 0.45877 | Regression loss: 0.39935 | Running loss: 1.63856\n",
      "Epoch: 1 | Iteration: 28 | Classification loss: 0.63091 | Regression loss: 1.12803 | Running loss: 1.63882\n",
      "Epoch: 1 | Iteration: 29 | Classification loss: 0.70544 | Regression loss: 0.75868 | Running loss: 1.63845\n",
      "Epoch: 1 | Iteration: 30 | Classification loss: 0.78351 | Regression loss: 0.79239 | Running loss: 1.63831\n",
      "Epoch: 1 | Iteration: 31 | Classification loss: 0.66141 | Regression loss: 0.58486 | Running loss: 1.63747\n",
      "Epoch: 1 | Iteration: 32 | Classification loss: 0.75902 | Regression loss: 0.83393 | Running loss: 1.63737\n",
      "Epoch: 1 | Iteration: 33 | Classification loss: 0.62228 | Regression loss: 0.86136 | Running loss: 1.63704\n",
      "Epoch: 1 | Iteration: 34 | Classification loss: 0.87959 | Regression loss: 0.85438 | Running loss: 1.63725\n",
      "Epoch: 1 | Iteration: 35 | Classification loss: 0.76163 | Regression loss: 0.70696 | Running loss: 1.63689\n",
      "Epoch: 1 | Iteration: 36 | Classification loss: 0.46228 | Regression loss: 0.31808 | Running loss: 1.63507\n",
      "Epoch: 1 | Iteration: 37 | Classification loss: 1.66473 | Regression loss: 0.39638 | Running loss: 1.63597\n",
      "Epoch: 1 | Iteration: 38 | Classification loss: 0.62712 | Regression loss: 0.40586 | Running loss: 1.63469\n",
      "Epoch: 1 | Iteration: 39 | Classification loss: 0.64387 | Regression loss: 0.96956 | Running loss: 1.63465\n",
      "Epoch: 1 | Iteration: 40 | Classification loss: 0.73209 | Regression loss: 0.74049 | Running loss: 1.63431\n",
      "Epoch: 1 | Iteration: 41 | Classification loss: 0.76250 | Regression loss: 1.17243 | Running loss: 1.63494\n",
      "Epoch: 1 | Iteration: 42 | Classification loss: 0.36891 | Regression loss: 0.21765 | Running loss: 1.63274\n",
      "Epoch: 1 | Iteration: 43 | Classification loss: 0.85354 | Regression loss: 1.01095 | Running loss: 1.63322\n",
      "Epoch: 1 | Iteration: 44 | Classification loss: 0.28924 | Regression loss: 0.51210 | Running loss: 1.63148\n",
      "Epoch: 1 | Iteration: 45 | Classification loss: 0.78676 | Regression loss: 1.17212 | Running loss: 1.63217\n",
      "Epoch: 1 | Iteration: 46 | Classification loss: 0.54308 | Regression loss: 0.95644 | Running loss: 1.63189\n",
      "Epoch: 1 | Iteration: 47 | Classification loss: 0.66706 | Regression loss: 0.38733 | Running loss: 1.63069\n",
      "Epoch: 1 | Iteration: 48 | Classification loss: 0.65408 | Regression loss: 0.85756 | Running loss: 1.63044\n",
      "Epoch: 1 | Iteration: 49 | Classification loss: 0.60220 | Regression loss: 1.04322 | Running loss: 1.63047\n",
      "Epoch: 1 | Iteration: 50 | Classification loss: 0.78669 | Regression loss: 0.95557 | Running loss: 1.63071\n",
      "Epoch: 1 | Iteration: 51 | Classification loss: 0.51465 | Regression loss: 0.91582 | Running loss: 1.63029\n",
      "Epoch: 1 | Iteration: 52 | Classification loss: 0.70338 | Regression loss: 0.74086 | Running loss: 1.62991\n",
      "Epoch: 1 | Iteration: 53 | Classification loss: 1.23789 | Regression loss: 0.96754 | Running loss: 1.63109\n",
      "Epoch: 1 | Iteration: 54 | Classification loss: 0.94424 | Regression loss: 0.79856 | Running loss: 1.63132\n",
      "Epoch: 1 | Iteration: 55 | Classification loss: 0.93732 | Regression loss: 0.70965 | Running loss: 1.63135\n",
      "Epoch: 1 | Iteration: 56 | Classification loss: 0.61671 | Regression loss: 0.87817 | Running loss: 1.63107\n",
      "Epoch: 1 | Iteration: 57 | Classification loss: 0.49440 | Regression loss: 0.96607 | Running loss: 1.63073\n",
      "Epoch: 1 | Iteration: 58 | Classification loss: 0.78614 | Regression loss: 1.07080 | Running loss: 1.63119\n",
      "Epoch: 1 | Iteration: 59 | Classification loss: 0.66564 | Regression loss: 0.98248 | Running loss: 1.63122\n",
      "Epoch: 1 | Iteration: 60 | Classification loss: 0.66350 | Regression loss: 0.58362 | Running loss: 1.63044\n",
      "Epoch: 1 | Iteration: 61 | Classification loss: 0.55775 | Regression loss: 0.95508 | Running loss: 1.63021\n",
      "Epoch: 1 | Iteration: 62 | Classification loss: 0.48543 | Regression loss: 0.73178 | Running loss: 1.62937\n",
      "Epoch: 1 | Iteration: 63 | Classification loss: 0.69767 | Regression loss: 0.74482 | Running loss: 1.62900\n",
      "Epoch: 1 | Iteration: 64 | Classification loss: 0.63882 | Regression loss: 0.67690 | Running loss: 1.62837\n",
      "Epoch: 1 | Iteration: 65 | Classification loss: 0.74067 | Regression loss: 0.93060 | Running loss: 1.62845\n",
      "Epoch: 1 | Iteration: 66 | Classification loss: 0.86125 | Regression loss: 0.27825 | Running loss: 1.62748\n",
      "Epoch: 1 | Iteration: 67 | Classification loss: 0.72116 | Regression loss: 0.79123 | Running loss: 1.62619\n",
      "Epoch: 1 | Iteration: 68 | Classification loss: 0.57391 | Regression loss: 1.03861 | Running loss: 1.62721\n",
      "Epoch: 1 | Iteration: 69 | Classification loss: 1.25339 | Regression loss: 0.00000 | Running loss: 1.62576\n",
      "Epoch: 1 | Iteration: 70 | Classification loss: 0.69536 | Regression loss: 0.95104 | Running loss: 1.62467\n",
      "Epoch: 1 | Iteration: 71 | Classification loss: 0.62414 | Regression loss: 0.96296 | Running loss: 1.62349\n",
      "Epoch: 1 | Iteration: 72 | Classification loss: 1.78737 | Regression loss: 0.42783 | Running loss: 1.62349\n",
      "Epoch: 1 | Iteration: 73 | Classification loss: 0.58789 | Regression loss: 0.75217 | Running loss: 1.62184\n",
      "Epoch: 1 | Iteration: 74 | Classification loss: 0.70397 | Regression loss: 0.92436 | Running loss: 1.62091\n",
      "Epoch: 1 | Iteration: 75 | Classification loss: 0.34565 | Regression loss: 0.47427 | Running loss: 1.61807\n",
      "Epoch: 1 | Iteration: 76 | Classification loss: 0.61292 | Regression loss: 0.85383 | Running loss: 1.61676\n",
      "Epoch: 1 | Iteration: 77 | Classification loss: 0.54620 | Regression loss: 0.88080 | Running loss: 1.61560\n",
      "Epoch: 1 | Iteration: 78 | Classification loss: 1.04882 | Regression loss: 0.39209 | Running loss: 1.61417\n",
      "Epoch: 1 | Iteration: 79 | Classification loss: 0.90494 | Regression loss: 1.00544 | Running loss: 1.61427\n",
      "Epoch: 1 | Iteration: 80 | Classification loss: 0.70996 | Regression loss: 0.93025 | Running loss: 1.61503\n",
      "Epoch: 1 | Iteration: 81 | Classification loss: 0.62309 | Regression loss: 1.10708 | Running loss: 1.61432\n",
      "Epoch: 1 | Iteration: 82 | Classification loss: 0.38532 | Regression loss: 0.30623 | Running loss: 1.61175\n",
      "Epoch: 1 | Iteration: 83 | Classification loss: 0.81397 | Regression loss: 0.88956 | Running loss: 1.61104\n",
      "Epoch: 1 | Iteration: 84 | Classification loss: 0.63543 | Regression loss: 0.82329 | Running loss: 1.60951\n",
      "Epoch: 1 | Iteration: 85 | Classification loss: 0.65229 | Regression loss: 0.96412 | Running loss: 1.60906\n",
      "Epoch: 1 | Iteration: 86 | Classification loss: 0.91225 | Regression loss: 0.65146 | Running loss: 1.61011\n",
      "Epoch: 1 | Iteration: 87 | Classification loss: 0.70180 | Regression loss: 0.84736 | Running loss: 1.60980\n",
      "Epoch: 1 | Iteration: 88 | Classification loss: 0.72077 | Regression loss: 0.78597 | Running loss: 1.60871\n",
      "Epoch: 1 | Iteration: 89 | Classification loss: 0.54439 | Regression loss: 0.77604 | Running loss: 1.60717\n",
      "Epoch: 1 | Iteration: 90 | Classification loss: 1.01432 | Regression loss: 1.05947 | Running loss: 1.60777\n",
      "Epoch: 1 | Iteration: 91 | Classification loss: 0.74838 | Regression loss: 1.02931 | Running loss: 1.60721\n",
      "Epoch: 1 | Iteration: 92 | Classification loss: 0.62808 | Regression loss: 0.88075 | Running loss: 1.60670\n",
      "Epoch: 1 | Iteration: 93 | Classification loss: 0.58148 | Regression loss: 0.74802 | Running loss: 1.60527\n",
      "Epoch: 1 | Iteration: 94 | Classification loss: 0.72831 | Regression loss: 1.12515 | Running loss: 1.60503\n",
      "Epoch: 1 | Iteration: 95 | Classification loss: 0.59342 | Regression loss: 0.77362 | Running loss: 1.60421\n",
      "Epoch: 1 | Iteration: 96 | Classification loss: 0.51674 | Regression loss: 0.91852 | Running loss: 1.60315\n",
      "Epoch: 1 | Iteration: 97 | Classification loss: 0.81107 | Regression loss: 0.91067 | Running loss: 1.60277\n",
      "Epoch: 1 | Iteration: 98 | Classification loss: 0.52650 | Regression loss: 0.91437 | Running loss: 1.60201\n",
      "Epoch: 1 | Iteration: 99 | Classification loss: 0.69287 | Regression loss: 0.77314 | Running loss: 1.60112\n",
      "Epoch: 1 | Iteration: 100 | Classification loss: 0.93770 | Regression loss: 0.62938 | Running loss: 1.60028\n",
      "Epoch: 1 | Iteration: 101 | Classification loss: 0.54548 | Regression loss: 0.94639 | Running loss: 1.60064\n",
      "Epoch: 1 | Iteration: 102 | Classification loss: 0.63692 | Regression loss: 0.81619 | Running loss: 1.59951\n",
      "Epoch: 1 | Iteration: 103 | Classification loss: 0.80032 | Regression loss: 0.87062 | Running loss: 1.59909\n",
      "Epoch: 1 | Iteration: 104 | Classification loss: 0.62784 | Regression loss: 1.00106 | Running loss: 1.59887\n",
      "Epoch: 1 | Iteration: 105 | Classification loss: 0.68033 | Regression loss: 0.71850 | Running loss: 1.59783\n",
      "Epoch: 1 | Iteration: 106 | Classification loss: 0.92035 | Regression loss: 0.90511 | Running loss: 1.59790\n",
      "Epoch: 1 | Iteration: 107 | Classification loss: 0.51372 | Regression loss: 0.89712 | Running loss: 1.59739\n",
      "Epoch: 1 | Iteration: 108 | Classification loss: 0.53990 | Regression loss: 0.74706 | Running loss: 1.59611\n",
      "Epoch: 1 | Iteration: 109 | Classification loss: 0.29787 | Regression loss: 0.91177 | Running loss: 1.59519\n",
      "Epoch: 1 | Iteration: 110 | Classification loss: 0.70750 | Regression loss: 1.04759 | Running loss: 1.59482\n",
      "Epoch: 1 | Iteration: 111 | Classification loss: 0.87848 | Regression loss: 0.69588 | Running loss: 1.59432\n",
      "Epoch: 1 | Iteration: 112 | Classification loss: 0.73315 | Regression loss: 0.92349 | Running loss: 1.59382\n",
      "Epoch: 1 | Iteration: 113 | Classification loss: 0.73007 | Regression loss: 0.86370 | Running loss: 1.59348\n",
      "Epoch: 1 | Iteration: 114 | Classification loss: 2.00767 | Regression loss: 0.69794 | Running loss: 1.59544\n",
      "Epoch: 1 | Iteration: 115 | Classification loss: 0.76308 | Regression loss: 0.82786 | Running loss: 1.59501\n",
      "Epoch: 1 | Iteration: 116 | Classification loss: 0.66503 | Regression loss: 0.87077 | Running loss: 1.59447\n",
      "Epoch: 1 | Iteration: 117 | Classification loss: 0.79609 | Regression loss: 0.70231 | Running loss: 1.59479\n",
      "Epoch: 1 | Iteration: 118 | Classification loss: 0.89621 | Regression loss: 0.72647 | Running loss: 1.59506\n",
      "Epoch: 1 | Iteration: 119 | Classification loss: 0.55467 | Regression loss: 0.71650 | Running loss: 1.59416\n",
      "Epoch: 1 | Iteration: 120 | Classification loss: 0.59857 | Regression loss: 0.90879 | Running loss: 1.59488\n",
      "Epoch: 1 | Iteration: 121 | Classification loss: 0.46968 | Regression loss: 0.84320 | Running loss: 1.59376\n",
      "Epoch: 1 | Iteration: 122 | Classification loss: 0.53110 | Regression loss: 0.99589 | Running loss: 1.59332\n",
      "Epoch: 1 | Iteration: 123 | Classification loss: 0.71207 | Regression loss: 0.83995 | Running loss: 1.59270\n",
      "Epoch: 1 | Iteration: 124 | Classification loss: 0.22760 | Regression loss: 0.34955 | Running loss: 1.58999\n",
      "Epoch: 1 | Iteration: 125 | Classification loss: 0.46781 | Regression loss: 0.75941 | Running loss: 1.58872\n",
      "Epoch: 1 | Iteration: 126 | Classification loss: 2.34555 | Regression loss: 0.35774 | Running loss: 1.59073\n",
      "Epoch: 1 | Iteration: 127 | Classification loss: 0.58660 | Regression loss: 0.87673 | Running loss: 1.59202\n",
      "Epoch: 1 | Iteration: 128 | Classification loss: 0.70824 | Regression loss: 0.89117 | Running loss: 1.59274\n",
      "Epoch: 1 | Iteration: 129 | Classification loss: 0.69385 | Regression loss: 0.77839 | Running loss: 1.59179\n",
      "Epoch: 1 | Iteration: 130 | Classification loss: 0.83054 | Regression loss: 0.85763 | Running loss: 1.59160\n",
      "Epoch: 1 | Iteration: 131 | Classification loss: 0.42011 | Regression loss: 0.77933 | Running loss: 1.59057\n",
      "Epoch: 1 | Iteration: 132 | Classification loss: 0.12211 | Regression loss: 0.00000 | Running loss: 1.58834\n",
      "Epoch: 1 | Iteration: 133 | Classification loss: 0.58829 | Regression loss: 1.00380 | Running loss: 1.58784\n",
      "Epoch: 1 | Iteration: 134 | Classification loss: 0.76053 | Regression loss: 0.91155 | Running loss: 1.58716\n",
      "Epoch: 1 | Iteration: 135 | Classification loss: 0.91173 | Regression loss: 0.91964 | Running loss: 1.58861\n",
      "Epoch: 1 | Iteration: 136 | Classification loss: 0.46791 | Regression loss: 0.48030 | Running loss: 1.58647\n",
      "Epoch: 1 | Iteration: 137 | Classification loss: 0.85082 | Regression loss: 0.81706 | Running loss: 1.58603\n",
      "Epoch: 1 | Iteration: 138 | Classification loss: 1.07180 | Regression loss: 0.67202 | Running loss: 1.58638\n",
      "Epoch: 1 | Iteration: 139 | Classification loss: 0.47510 | Regression loss: 0.00000 | Running loss: 1.58400\n",
      "Epoch: 1 | Iteration: 140 | Classification loss: 0.49497 | Regression loss: 0.41615 | Running loss: 1.58241\n",
      "Epoch: 1 | Iteration: 141 | Classification loss: 0.60579 | Regression loss: 0.79618 | Running loss: 1.58399\n",
      "Epoch: 1 | Iteration: 142 | Classification loss: 0.82856 | Regression loss: 1.00207 | Running loss: 1.58341\n",
      "Epoch: 1 | Iteration: 143 | Classification loss: 0.79132 | Regression loss: 0.76231 | Running loss: 1.58301\n",
      "Epoch: 1 | Iteration: 144 | Classification loss: 0.77301 | Regression loss: 1.00264 | Running loss: 1.58590\n",
      "Epoch: 1 | Iteration: 145 | Classification loss: 0.75317 | Regression loss: 0.88303 | Running loss: 1.58535\n",
      "Epoch: 1 | Iteration: 146 | Classification loss: 0.94106 | Regression loss: 0.88825 | Running loss: 1.58553\n",
      "Epoch: 1 | Iteration: 147 | Classification loss: 0.69104 | Regression loss: 0.64198 | Running loss: 1.58441\n",
      "Epoch: 1 | Iteration: 148 | Classification loss: 0.63482 | Regression loss: 0.97558 | Running loss: 1.58456\n",
      "Epoch: 1 | Iteration: 149 | Classification loss: 0.85502 | Regression loss: 0.86268 | Running loss: 1.58418\n",
      "Epoch: 1 | Iteration: 150 | Classification loss: 0.63590 | Regression loss: 0.85804 | Running loss: 1.58357\n",
      "Epoch: 1 | Iteration: 151 | Classification loss: 0.58253 | Regression loss: 0.81955 | Running loss: 1.58305\n",
      "Epoch: 1 | Iteration: 152 | Classification loss: 1.70056 | Regression loss: 0.00000 | Running loss: 1.58216\n",
      "Epoch: 1 | Iteration: 153 | Classification loss: 0.55133 | Regression loss: 1.02990 | Running loss: 1.58176\n",
      "Epoch: 1 | Iteration: 154 | Classification loss: 0.67648 | Regression loss: 0.00000 | Running loss: 1.57940\n",
      "Epoch: 1 | Iteration: 155 | Classification loss: 0.58278 | Regression loss: 0.87258 | Running loss: 1.57823\n",
      "Epoch: 1 | Iteration: 156 | Classification loss: 0.79745 | Regression loss: 0.81658 | Running loss: 1.57751\n",
      "Epoch: 1 | Iteration: 157 | Classification loss: 0.78954 | Regression loss: 0.80908 | Running loss: 1.57689\n",
      "Epoch: 1 | Iteration: 158 | Classification loss: 1.17934 | Regression loss: 0.55324 | Running loss: 1.57679\n",
      "Epoch: 1 | Iteration: 159 | Classification loss: 0.68919 | Regression loss: 0.73102 | Running loss: 1.57593\n",
      "Epoch: 1 | Iteration: 160 | Classification loss: 0.46805 | Regression loss: 0.50710 | Running loss: 1.57294\n",
      "Epoch: 1 | Iteration: 161 | Classification loss: 0.64101 | Regression loss: 0.97801 | Running loss: 1.57305\n",
      "Epoch: 1 | Iteration: 162 | Classification loss: 0.84405 | Regression loss: 0.69696 | Running loss: 1.57295\n",
      "Epoch: 1 | Iteration: 163 | Classification loss: 0.61958 | Regression loss: 0.92973 | Running loss: 1.57300\n",
      "Epoch: 1 | Iteration: 164 | Classification loss: 0.58829 | Regression loss: 0.73471 | Running loss: 1.57217\n",
      "Epoch: 1 | Iteration: 165 | Classification loss: 0.70815 | Regression loss: 0.87762 | Running loss: 1.57271\n",
      "Epoch: 1 | Iteration: 166 | Classification loss: 0.63075 | Regression loss: 0.88875 | Running loss: 1.57219\n",
      "Epoch: 1 | Iteration: 167 | Classification loss: 0.68317 | Regression loss: 1.06314 | Running loss: 1.57246\n",
      "Epoch: 1 | Iteration: 168 | Classification loss: 0.26864 | Regression loss: 0.44756 | Running loss: 1.57159\n",
      "Epoch: 1 | Iteration: 169 | Classification loss: 0.78610 | Regression loss: 0.79516 | Running loss: 1.57133\n",
      "Epoch: 1 | Iteration: 170 | Classification loss: 1.02263 | Regression loss: 1.01638 | Running loss: 1.57218\n",
      "Epoch: 1 | Iteration: 171 | Classification loss: 1.15477 | Regression loss: 0.88645 | Running loss: 1.57246\n",
      "Epoch: 1 | Iteration: 172 | Classification loss: 0.73920 | Regression loss: 0.98080 | Running loss: 1.57219\n",
      "Epoch: 1 | Iteration: 173 | Classification loss: 0.41468 | Regression loss: 0.85901 | Running loss: 1.57112\n",
      "Epoch: 1 | Iteration: 174 | Classification loss: 0.45786 | Regression loss: 0.85075 | Running loss: 1.57064\n",
      "Epoch: 1 | Iteration: 175 | Classification loss: 0.65166 | Regression loss: 0.83216 | Running loss: 1.57002\n",
      "Epoch: 1 | Iteration: 176 | Classification loss: 1.01432 | Regression loss: 0.86156 | Running loss: 1.57047\n",
      "Epoch: 1 | Iteration: 177 | Classification loss: 0.39930 | Regression loss: 0.45489 | Running loss: 1.56887\n",
      "Epoch: 1 | Iteration: 178 | Classification loss: 0.70416 | Regression loss: 0.94888 | Running loss: 1.56816\n",
      "Epoch: 1 | Iteration: 179 | Classification loss: 0.67043 | Regression loss: 0.36435 | Running loss: 1.56689\n",
      "Epoch: 1 | Iteration: 180 | Classification loss: 0.71985 | Regression loss: 0.94259 | Running loss: 1.56664\n",
      "Epoch: 1 | Iteration: 181 | Classification loss: 0.57785 | Regression loss: 0.94062 | Running loss: 1.56749\n",
      "Epoch: 1 | Iteration: 182 | Classification loss: 0.67198 | Regression loss: 0.97808 | Running loss: 1.56749\n",
      "Epoch: 1 | Iteration: 183 | Classification loss: 0.70593 | Regression loss: 0.64838 | Running loss: 1.56702\n",
      "Epoch: 1 | Iteration: 184 | Classification loss: 1.24695 | Regression loss: 0.97295 | Running loss: 1.56768\n",
      "Epoch: 1 | Iteration: 185 | Classification loss: 0.56140 | Regression loss: 0.98914 | Running loss: 1.56715\n",
      "Epoch: 1 | Iteration: 186 | Classification loss: 0.55326 | Regression loss: 0.71309 | Running loss: 1.56617\n",
      "Epoch: 1 | Iteration: 187 | Classification loss: 0.78328 | Regression loss: 0.98682 | Running loss: 1.56590\n",
      "Epoch: 1 | Iteration: 188 | Classification loss: 0.73685 | Regression loss: 0.88516 | Running loss: 1.56566\n",
      "Epoch: 1 | Iteration: 189 | Classification loss: 0.52216 | Regression loss: 0.90636 | Running loss: 1.56636\n",
      "Epoch: 1 | Iteration: 190 | Classification loss: 0.61306 | Regression loss: 0.92298 | Running loss: 1.56731\n",
      "Epoch: 1 | Iteration: 191 | Classification loss: 0.64332 | Regression loss: 0.90262 | Running loss: 1.56676\n",
      "Epoch: 1 | Iteration: 192 | Classification loss: 0.80270 | Regression loss: 0.87662 | Running loss: 1.56685\n",
      "Epoch: 1 | Iteration: 193 | Classification loss: 0.51333 | Regression loss: 0.84715 | Running loss: 1.56617\n",
      "Epoch: 1 | Iteration: 194 | Classification loss: 0.49402 | Regression loss: 0.89356 | Running loss: 1.56541\n",
      "Epoch: 1 | Iteration: 195 | Classification loss: 0.72433 | Regression loss: 0.81884 | Running loss: 1.56530\n",
      "Epoch: 1 | Iteration: 196 | Classification loss: 0.60743 | Regression loss: 0.91924 | Running loss: 1.56461\n",
      "Epoch: 1 | Iteration: 197 | Classification loss: 0.64038 | Regression loss: 0.80835 | Running loss: 1.56583\n",
      "Epoch: 1 | Iteration: 198 | Classification loss: 0.69379 | Regression loss: 0.80267 | Running loss: 1.56554\n",
      "Epoch: 1 | Iteration: 199 | Classification loss: 0.66118 | Regression loss: 0.85998 | Running loss: 1.56649\n",
      "Epoch: 1 | Iteration: 200 | Classification loss: 0.67344 | Regression loss: 0.84741 | Running loss: 1.56586\n",
      "Epoch: 1 | Iteration: 201 | Classification loss: 0.72222 | Regression loss: 0.53694 | Running loss: 1.56490\n",
      "Epoch: 1 | Iteration: 202 | Classification loss: 0.81146 | Regression loss: 0.86380 | Running loss: 1.56503\n",
      "Epoch: 1 | Iteration: 203 | Classification loss: 0.49205 | Regression loss: 0.83536 | Running loss: 1.56438\n",
      "Epoch: 1 | Iteration: 204 | Classification loss: 0.71404 | Regression loss: 0.89916 | Running loss: 1.56418\n",
      "Epoch: 1 | Iteration: 205 | Classification loss: 0.65588 | Regression loss: 0.76034 | Running loss: 1.56362\n",
      "Epoch: 1 | Iteration: 206 | Classification loss: 0.63003 | Regression loss: 1.03825 | Running loss: 1.56342\n",
      "Epoch: 1 | Iteration: 207 | Classification loss: 0.49036 | Regression loss: 0.88974 | Running loss: 1.56418\n",
      "Epoch: 1 | Iteration: 208 | Classification loss: 5.86438 | Regression loss: 0.44202 | Running loss: 1.57494\n",
      "Epoch: 1 | Iteration: 209 | Classification loss: 0.66084 | Regression loss: 0.75530 | Running loss: 1.57537\n",
      "Epoch: 1 | Iteration: 210 | Classification loss: 1.15244 | Regression loss: 0.69704 | Running loss: 1.57543\n",
      "Epoch: 1 | Iteration: 211 | Classification loss: 0.67970 | Regression loss: 0.97075 | Running loss: 1.57546\n",
      "Epoch: 1 | Iteration: 212 | Classification loss: 0.65804 | Regression loss: 0.98234 | Running loss: 1.57547\n",
      "Epoch: 1 | Iteration: 213 | Classification loss: 0.67586 | Regression loss: 1.04429 | Running loss: 1.57568\n",
      "Epoch: 1 | Iteration: 214 | Classification loss: 0.90267 | Regression loss: 0.88848 | Running loss: 1.57570\n",
      "Epoch: 1 | Iteration: 215 | Classification loss: 0.71785 | Regression loss: 0.91110 | Running loss: 1.57502\n",
      "Epoch: 1 | Iteration: 216 | Classification loss: 0.96070 | Regression loss: 0.84063 | Running loss: 1.57618\n",
      "Epoch: 1 | Iteration: 217 | Classification loss: 0.51585 | Regression loss: 0.46938 | Running loss: 1.57572\n",
      "Epoch: 1 | Iteration: 218 | Classification loss: 0.74578 | Regression loss: 0.78841 | Running loss: 1.57546\n",
      "Epoch: 1 | Iteration: 219 | Classification loss: 0.60522 | Regression loss: 0.75749 | Running loss: 1.57452\n",
      "Epoch: 1 | Iteration: 220 | Classification loss: 0.66849 | Regression loss: 0.88482 | Running loss: 1.57538\n",
      "Epoch: 1 | Iteration: 221 | Classification loss: 0.72111 | Regression loss: 0.87697 | Running loss: 1.57503\n",
      "Epoch: 1 | Iteration: 222 | Classification loss: 0.60585 | Regression loss: 1.02024 | Running loss: 1.57467\n",
      "Epoch: 1 | Iteration: 223 | Classification loss: 0.65021 | Regression loss: 0.97497 | Running loss: 1.57431\n",
      "Epoch: 1 | Iteration: 224 | Classification loss: 0.13664 | Regression loss: 0.00000 | Running loss: 1.57100\n",
      "Epoch: 1 | Iteration: 225 | Classification loss: 0.66993 | Regression loss: 0.77827 | Running loss: 1.57048\n",
      "Epoch: 1 | Iteration: 226 | Classification loss: 0.87414 | Regression loss: 0.63889 | Running loss: 1.57065\n",
      "Epoch: 1 | Iteration: 227 | Classification loss: 0.50017 | Regression loss: 0.82234 | Running loss: 1.56902\n",
      "Epoch: 1 | Iteration: 228 | Classification loss: 0.47946 | Regression loss: 0.93007 | Running loss: 1.56873\n",
      "Epoch: 1 | Iteration: 229 | Classification loss: 1.68763 | Regression loss: 0.43020 | Running loss: 1.56981\n",
      "Epoch: 1 | Iteration: 230 | Classification loss: 0.59949 | Regression loss: 0.44631 | Running loss: 1.56896\n",
      "Epoch: 1 | Iteration: 231 | Classification loss: 0.51998 | Regression loss: 0.84419 | Running loss: 1.56845\n",
      "Epoch: 1 | Iteration: 232 | Classification loss: 0.41932 | Regression loss: 0.41319 | Running loss: 1.56634\n",
      "Epoch: 1 | Iteration: 233 | Classification loss: 0.65460 | Regression loss: 1.03862 | Running loss: 1.56637\n",
      "Epoch: 1 | Iteration: 234 | Classification loss: 0.61525 | Regression loss: 1.00437 | Running loss: 1.56641\n",
      "Epoch: 1 | Iteration: 235 | Classification loss: 0.86660 | Regression loss: 0.96948 | Running loss: 1.56698\n",
      "Epoch: 1 | Iteration: 236 | Classification loss: 0.62858 | Regression loss: 0.74169 | Running loss: 1.56666\n",
      "Epoch: 1 | Iteration: 237 | Classification loss: 0.73158 | Regression loss: 0.84156 | Running loss: 1.56680\n",
      "Epoch: 1 | Iteration: 238 | Classification loss: 0.75144 | Regression loss: 0.95518 | Running loss: 1.56635\n",
      "Epoch: 1 | Iteration: 239 | Classification loss: 0.72784 | Regression loss: 0.87952 | Running loss: 1.56626\n",
      "Epoch: 1 | Iteration: 240 | Classification loss: 0.54352 | Regression loss: 0.80521 | Running loss: 1.56596\n",
      "Epoch: 1 | Iteration: 241 | Classification loss: 0.44359 | Regression loss: 0.83773 | Running loss: 1.56514\n",
      "Epoch: 1 | Iteration: 242 | Classification loss: 0.53181 | Regression loss: 1.10256 | Running loss: 1.56583\n",
      "Epoch: 1 | Iteration: 243 | Classification loss: 0.99048 | Regression loss: 0.85968 | Running loss: 1.56613\n",
      "Epoch: 1 | Iteration: 244 | Classification loss: 1.56016 | Regression loss: 0.44094 | Running loss: 1.56588\n",
      "Epoch: 1 | Iteration: 245 | Classification loss: 0.53459 | Regression loss: 0.73535 | Running loss: 1.56626\n",
      "Epoch: 1 | Iteration: 246 | Classification loss: 0.84791 | Regression loss: 0.77867 | Running loss: 1.56646\n",
      "Epoch: 1 | Iteration: 247 | Classification loss: 0.70325 | Regression loss: 0.77152 | Running loss: 1.56579\n",
      "Epoch: 1 | Iteration: 248 | Classification loss: 0.84792 | Regression loss: 0.86199 | Running loss: 1.56547\n",
      "Epoch: 1 | Iteration: 249 | Classification loss: 0.76504 | Regression loss: 0.41448 | Running loss: 1.56597\n",
      "Epoch: 1 | Iteration: 250 | Classification loss: 0.62378 | Regression loss: 0.87991 | Running loss: 1.56508\n",
      "Epoch: 1 | Iteration: 251 | Classification loss: 0.82541 | Regression loss: 0.91520 | Running loss: 1.56524\n",
      "Epoch: 1 | Iteration: 252 | Classification loss: 0.61926 | Regression loss: 0.86027 | Running loss: 1.56475\n",
      "Epoch: 1 | Iteration: 253 | Classification loss: 0.83842 | Regression loss: 0.94869 | Running loss: 1.56454\n",
      "Epoch: 1 | Iteration: 254 | Classification loss: 0.77427 | Regression loss: 0.90642 | Running loss: 1.56743\n",
      "Epoch: 1 | Iteration: 255 | Classification loss: 0.57454 | Regression loss: 0.84632 | Running loss: 1.56686\n",
      "Epoch: 1 | Iteration: 256 | Classification loss: 0.68051 | Regression loss: 0.93605 | Running loss: 1.56623\n",
      "Epoch: 1 | Iteration: 257 | Classification loss: 0.86184 | Regression loss: 0.83018 | Running loss: 1.56661\n",
      "Epoch: 1 | Iteration: 258 | Classification loss: 0.32821 | Regression loss: 0.00000 | Running loss: 1.56422\n",
      "Epoch: 1 | Iteration: 259 | Classification loss: 0.62691 | Regression loss: 1.02644 | Running loss: 1.56398\n",
      "Epoch: 1 | Iteration: 260 | Classification loss: 1.40545 | Regression loss: 0.29706 | Running loss: 1.56330\n",
      "Epoch: 1 | Iteration: 261 | Classification loss: 0.91884 | Regression loss: 0.75925 | Running loss: 1.56350\n",
      "Epoch: 1 | Iteration: 262 | Classification loss: 0.77495 | Regression loss: 0.95780 | Running loss: 1.56319\n",
      "Epoch: 1 | Iteration: 263 | Classification loss: 0.43487 | Regression loss: 0.29733 | Running loss: 1.56144\n",
      "Epoch: 1 | Iteration: 264 | Classification loss: 0.74965 | Regression loss: 0.84309 | Running loss: 1.56244\n",
      "Epoch: 1 | Iteration: 265 | Classification loss: 0.53960 | Regression loss: 0.47787 | Running loss: 1.56069\n",
      "Epoch: 1 | Iteration: 266 | Classification loss: 0.63839 | Regression loss: 0.91098 | Running loss: 1.56028\n",
      "Epoch: 1 | Iteration: 267 | Classification loss: 0.56831 | Regression loss: 1.00452 | Running loss: 1.56017\n",
      "Epoch: 1 | Iteration: 268 | Classification loss: 0.65145 | Regression loss: 0.76296 | Running loss: 1.56087\n",
      "Epoch: 1 | Iteration: 269 | Classification loss: 0.77040 | Regression loss: 0.88784 | Running loss: 1.56084\n",
      "Epoch: 1 | Iteration: 270 | Classification loss: 0.34894 | Regression loss: 0.48830 | Running loss: 1.55890\n",
      "Epoch: 1 | Iteration: 271 | Classification loss: 0.41821 | Regression loss: 0.29771 | Running loss: 1.55718\n",
      "Epoch: 1 | Iteration: 272 | Classification loss: 0.61261 | Regression loss: 0.94687 | Running loss: 1.55680\n",
      "Epoch: 1 | Iteration: 273 | Classification loss: 0.46186 | Regression loss: 0.31253 | Running loss: 1.55469\n",
      "Epoch: 1 | Iteration: 274 | Classification loss: 0.62718 | Regression loss: 0.78370 | Running loss: 1.55421\n",
      "Epoch: 1 | Iteration: 275 | Classification loss: 0.55520 | Regression loss: 0.82919 | Running loss: 1.55375\n",
      "Epoch: 1 | Iteration: 276 | Classification loss: 0.54197 | Regression loss: 0.81632 | Running loss: 1.55374\n",
      "Epoch: 1 | Iteration: 277 | Classification loss: 0.61553 | Regression loss: 0.95797 | Running loss: 1.55358\n",
      "Epoch: 1 | Iteration: 278 | Classification loss: 0.34475 | Regression loss: 0.90282 | Running loss: 1.55245\n",
      "Epoch: 1 | Iteration: 279 | Classification loss: 1.11586 | Regression loss: 0.89098 | Running loss: 1.55327\n",
      "Epoch: 1 | Iteration: 280 | Classification loss: 0.44029 | Regression loss: 0.88529 | Running loss: 1.55278\n",
      "Epoch: 1 | Iteration: 281 | Classification loss: 0.50457 | Regression loss: 0.97056 | Running loss: 1.55253\n",
      "Epoch: 1 | Iteration: 282 | Classification loss: 0.61776 | Regression loss: 0.80973 | Running loss: 1.55161\n",
      "Epoch: 1 | Iteration: 283 | Classification loss: 0.62140 | Regression loss: 0.00000 | Running loss: 1.54939\n",
      "Epoch: 1 | Iteration: 284 | Classification loss: 0.67696 | Regression loss: 0.82164 | Running loss: 1.54899\n",
      "Epoch: 1 | Iteration: 285 | Classification loss: 0.64435 | Regression loss: 0.89247 | Running loss: 1.54882\n",
      "Epoch: 1 | Iteration: 286 | Classification loss: 0.73758 | Regression loss: 0.91336 | Running loss: 1.54825\n",
      "Epoch: 1 | Iteration: 287 | Classification loss: 0.35994 | Regression loss: 0.89469 | Running loss: 1.54761\n",
      "Epoch: 1 | Iteration: 288 | Classification loss: 0.53591 | Regression loss: 1.00962 | Running loss: 1.54722\n",
      "Epoch: 1 | Iteration: 289 | Classification loss: 1.05105 | Regression loss: 0.99375 | Running loss: 1.54776\n",
      "Epoch: 1 | Iteration: 290 | Classification loss: 0.67221 | Regression loss: 0.95194 | Running loss: 1.54749\n",
      "Epoch: 1 | Iteration: 291 | Classification loss: 0.63881 | Regression loss: 0.82528 | Running loss: 1.54710\n",
      "Epoch: 1 | Iteration: 292 | Classification loss: 0.53996 | Regression loss: 0.36990 | Running loss: 1.54505\n",
      "Epoch: 1 | Iteration: 293 | Classification loss: 0.52989 | Regression loss: 0.77483 | Running loss: 1.54393\n",
      "Epoch: 1 | Iteration: 294 | Classification loss: 0.38916 | Regression loss: 0.89655 | Running loss: 1.54324\n",
      "Epoch: 1 | Iteration: 295 | Classification loss: 0.32457 | Regression loss: 0.75221 | Running loss: 1.54231\n",
      "Epoch: 1 | Iteration: 296 | Classification loss: 0.54326 | Regression loss: 0.94335 | Running loss: 1.54160\n",
      "Epoch: 1 | Iteration: 297 | Classification loss: 1.03088 | Regression loss: 0.43908 | Running loss: 1.54097\n",
      "Epoch: 1 | Iteration: 298 | Classification loss: 0.55102 | Regression loss: 0.91733 | Running loss: 1.54051\n",
      "Epoch: 1 | Iteration: 299 | Classification loss: 0.67805 | Regression loss: 0.67619 | Running loss: 1.53970\n",
      "Epoch: 1 | Iteration: 300 | Classification loss: 0.59871 | Regression loss: 0.98558 | Running loss: 1.53977\n",
      "Epoch: 1 | Iteration: 301 | Classification loss: 0.83871 | Regression loss: 0.82725 | Running loss: 1.53993\n",
      "Epoch: 1 | Iteration: 302 | Classification loss: 0.58719 | Regression loss: 0.81643 | Running loss: 1.53940\n",
      "Epoch: 1 | Iteration: 303 | Classification loss: 0.27219 | Regression loss: 0.36605 | Running loss: 1.53536\n",
      "Epoch: 1 | Iteration: 304 | Classification loss: 0.77186 | Regression loss: 0.80039 | Running loss: 1.53562\n",
      "Epoch: 1 | Iteration: 305 | Classification loss: 0.78990 | Regression loss: 0.87577 | Running loss: 1.53665\n",
      "Epoch: 1 | Iteration: 306 | Classification loss: 0.51894 | Regression loss: 0.93304 | Running loss: 1.53640\n",
      "Epoch: 1 | Iteration: 307 | Classification loss: 1.01093 | Regression loss: 0.86731 | Running loss: 1.53646\n",
      "Epoch: 1 | Iteration: 308 | Classification loss: 0.86112 | Regression loss: 0.29366 | Running loss: 1.53544\n",
      "Epoch: 1 | Iteration: 309 | Classification loss: 0.38448 | Regression loss: 0.83291 | Running loss: 1.53454\n",
      "Epoch: 1 | Iteration: 310 | Classification loss: 0.86129 | Regression loss: 0.67726 | Running loss: 1.53409\n",
      "Epoch: 1 | Iteration: 311 | Classification loss: 0.54770 | Regression loss: 0.80280 | Running loss: 1.53306\n",
      "Epoch: 1 | Iteration: 312 | Classification loss: 0.66769 | Regression loss: 0.79722 | Running loss: 1.53266\n",
      "Epoch: 1 | Iteration: 313 | Classification loss: 0.80932 | Regression loss: 0.85075 | Running loss: 1.53365\n",
      "Epoch: 1 | Iteration: 314 | Classification loss: 0.89177 | Regression loss: 0.90576 | Running loss: 1.53309\n",
      "Epoch: 1 | Iteration: 315 | Classification loss: 0.52061 | Regression loss: 0.85664 | Running loss: 1.53227\n",
      "Epoch: 1 | Iteration: 316 | Classification loss: 0.65990 | Regression loss: 0.91125 | Running loss: 1.53199\n",
      "Epoch: 1 | Iteration: 317 | Classification loss: 0.87061 | Regression loss: 0.99404 | Running loss: 1.53363\n",
      "Epoch: 1 | Iteration: 318 | Classification loss: 0.83784 | Regression loss: 0.27993 | Running loss: 1.53192\n",
      "Epoch: 1 | Iteration: 319 | Classification loss: 0.50191 | Regression loss: 0.68097 | Running loss: 1.53213\n",
      "Epoch: 1 | Iteration: 320 | Classification loss: 1.23850 | Regression loss: 1.03316 | Running loss: 1.53340\n",
      "Epoch: 1 | Iteration: 321 | Classification loss: 0.75571 | Regression loss: 0.90674 | Running loss: 1.53332\n",
      "Epoch: 1 | Iteration: 322 | Classification loss: 0.73679 | Regression loss: 0.76588 | Running loss: 1.53415\n",
      "Epoch: 1 | Iteration: 323 | Classification loss: 0.58377 | Regression loss: 0.85777 | Running loss: 1.53346\n",
      "Epoch: 1 | Iteration: 324 | Classification loss: 1.02576 | Regression loss: 0.88762 | Running loss: 1.53437\n",
      "Epoch: 1 | Iteration: 325 | Classification loss: 0.78649 | Regression loss: 0.82892 | Running loss: 1.53403\n",
      "Epoch: 1 | Iteration: 326 | Classification loss: 0.61635 | Regression loss: 0.45587 | Running loss: 1.53276\n",
      "Epoch: 1 | Iteration: 327 | Classification loss: 0.52681 | Regression loss: 0.79743 | Running loss: 1.53163\n",
      "Epoch: 1 | Iteration: 328 | Classification loss: 0.78350 | Regression loss: 0.91582 | Running loss: 1.53173\n",
      "Epoch: 1 | Iteration: 329 | Classification loss: 0.55275 | Regression loss: 0.66477 | Running loss: 1.53092\n",
      "Epoch: 1 | Iteration: 330 | Classification loss: 0.62127 | Regression loss: 0.90868 | Running loss: 1.53058\n",
      "Epoch: 1 | Iteration: 331 | Classification loss: 0.80236 | Regression loss: 0.81249 | Running loss: 1.53118\n",
      "Epoch: 1 | Iteration: 332 | Classification loss: 0.87904 | Regression loss: 0.83725 | Running loss: 1.53100\n",
      "Epoch: 1 | Iteration: 333 | Classification loss: 0.64118 | Regression loss: 0.83582 | Running loss: 1.53048\n",
      "Epoch: 1 | Iteration: 334 | Classification loss: 0.64081 | Regression loss: 1.05131 | Running loss: 1.53137\n",
      "Epoch: 1 | Iteration: 335 | Classification loss: 0.68224 | Regression loss: 0.88139 | Running loss: 1.53089\n",
      "Epoch: 1 | Iteration: 336 | Classification loss: 0.77163 | Regression loss: 0.78177 | Running loss: 1.53105\n",
      "Epoch: 1 | Iteration: 337 | Classification loss: 0.64200 | Regression loss: 0.96227 | Running loss: 1.53093\n",
      "Epoch: 1 | Iteration: 338 | Classification loss: 0.36853 | Regression loss: 0.38585 | Running loss: 1.52869\n",
      "Epoch: 1 | Iteration: 339 | Classification loss: 0.59649 | Regression loss: 0.92740 | Running loss: 1.52825\n",
      "Epoch: 1 | Iteration: 340 | Classification loss: 0.58482 | Regression loss: 0.73340 | Running loss: 1.52731\n",
      "Epoch: 1 | Iteration: 341 | Classification loss: 0.56520 | Regression loss: 0.97233 | Running loss: 1.52698\n",
      "Epoch: 1 | Iteration: 342 | Classification loss: 0.55767 | Regression loss: 0.88733 | Running loss: 1.52662\n",
      "Epoch: 1 | Iteration: 343 | Classification loss: 0.61281 | Regression loss: 0.37746 | Running loss: 1.52545\n",
      "Epoch: 1 | Iteration: 344 | Classification loss: 0.68039 | Regression loss: 0.75774 | Running loss: 1.52531\n",
      "Epoch: 1 | Iteration: 345 | Classification loss: 0.64158 | Regression loss: 0.90400 | Running loss: 1.52477\n",
      "Epoch: 1 | Iteration: 346 | Classification loss: 0.44366 | Regression loss: 0.87661 | Running loss: 1.52329\n",
      "Epoch: 1 | Iteration: 347 | Classification loss: 0.44487 | Regression loss: 0.40849 | Running loss: 1.52183\n",
      "Epoch: 1 | Iteration: 348 | Classification loss: 0.80715 | Regression loss: 0.89251 | Running loss: 1.52199\n",
      "Epoch: 1 | Iteration: 349 | Classification loss: 0.68314 | Regression loss: 0.89044 | Running loss: 1.52210\n",
      "Epoch: 1 | Iteration: 350 | Classification loss: 0.71952 | Regression loss: 0.86310 | Running loss: 1.52157\n",
      "Epoch: 1 | Iteration: 351 | Classification loss: 0.28967 | Regression loss: 0.43071 | Running loss: 1.51973\n",
      "Epoch: 1 | Iteration: 352 | Classification loss: 0.85792 | Regression loss: 1.05574 | Running loss: 1.52031\n",
      "Epoch: 1 | Iteration: 353 | Classification loss: 0.48669 | Regression loss: 0.88549 | Running loss: 1.52053\n",
      "Epoch: 1 | Iteration: 354 | Classification loss: 0.34232 | Regression loss: 0.64989 | Running loss: 1.51890\n",
      "Epoch: 1 | Iteration: 355 | Classification loss: 0.38304 | Regression loss: 0.34426 | Running loss: 1.51808\n",
      "Epoch: 1 | Iteration: 356 | Classification loss: 0.44122 | Regression loss: 0.75561 | Running loss: 1.51686\n",
      "Epoch: 1 | Iteration: 357 | Classification loss: 0.96460 | Regression loss: 0.81398 | Running loss: 1.51725\n",
      "Epoch: 1 | Iteration: 358 | Classification loss: 0.57011 | Regression loss: 0.83156 | Running loss: 1.51659\n",
      "Epoch: 1 | Iteration: 359 | Classification loss: 0.41128 | Regression loss: 0.42010 | Running loss: 1.51524\n",
      "Epoch: 1 | Iteration: 360 | Classification loss: 0.72754 | Regression loss: 0.64355 | Running loss: 1.51582\n",
      "Epoch: 1 | Iteration: 361 | Classification loss: 0.55358 | Regression loss: 0.43436 | Running loss: 1.51396\n",
      "Epoch: 1 | Iteration: 362 | Classification loss: 0.89716 | Regression loss: 0.76241 | Running loss: 1.51348\n",
      "Epoch: 1 | Iteration: 363 | Classification loss: 0.83979 | Regression loss: 0.82553 | Running loss: 1.51505\n",
      "Epoch: 1 | Iteration: 364 | Classification loss: 0.69103 | Regression loss: 0.84009 | Running loss: 1.51466\n",
      "Epoch: 1 | Iteration: 365 | Classification loss: 0.72951 | Regression loss: 0.94119 | Running loss: 1.51422\n",
      "Epoch: 1 | Iteration: 366 | Classification loss: 0.68255 | Regression loss: 1.04092 | Running loss: 1.51561\n",
      "Epoch: 1 | Iteration: 367 | Classification loss: 0.72147 | Regression loss: 0.73423 | Running loss: 1.51510\n",
      "Epoch: 1 | Iteration: 368 | Classification loss: 0.51433 | Regression loss: 0.66110 | Running loss: 1.51468\n",
      "Epoch: 1 | Iteration: 369 | Classification loss: 1.10709 | Regression loss: 0.00000 | Running loss: 1.51373\n",
      "Epoch: 1 | Iteration: 370 | Classification loss: 0.67930 | Regression loss: 1.00261 | Running loss: 1.51392\n",
      "Epoch: 1 | Iteration: 371 | Classification loss: 0.56740 | Regression loss: 0.75082 | Running loss: 1.51365\n",
      "Epoch: 1 | Iteration: 372 | Classification loss: 0.47005 | Regression loss: 0.81097 | Running loss: 1.51237\n",
      "Epoch: 1 | Iteration: 373 | Classification loss: 0.54324 | Regression loss: 0.85996 | Running loss: 1.51167\n",
      "Epoch: 1 | Iteration: 374 | Classification loss: 0.78811 | Regression loss: 0.84952 | Running loss: 1.51181\n",
      "Epoch: 1 | Iteration: 375 | Classification loss: 0.45408 | Regression loss: 0.81618 | Running loss: 1.51148\n",
      "Epoch: 1 | Iteration: 376 | Classification loss: 0.96259 | Regression loss: 1.04877 | Running loss: 1.51232\n",
      "Epoch: 1 | Iteration: 377 | Classification loss: 0.34892 | Regression loss: 0.66846 | Running loss: 1.51134\n",
      "Epoch: 1 | Iteration: 378 | Classification loss: 0.53382 | Regression loss: 1.03286 | Running loss: 1.51116\n",
      "Epoch: 1 | Iteration: 379 | Classification loss: 0.48999 | Regression loss: 0.88830 | Running loss: 1.51061\n",
      "Epoch: 1 | Iteration: 380 | Classification loss: 0.36899 | Regression loss: 0.85191 | Running loss: 1.50987\n",
      "Epoch: 1 | Iteration: 381 | Classification loss: 0.91850 | Regression loss: 0.36529 | Running loss: 1.50929\n",
      "Epoch: 1 | Iteration: 382 | Classification loss: 0.82269 | Regression loss: 0.91418 | Running loss: 1.50970\n",
      "Epoch: 1 | Iteration: 383 | Classification loss: 0.68217 | Regression loss: 0.81861 | Running loss: 1.50917\n",
      "Epoch: 1 | Iteration: 384 | Classification loss: 0.45627 | Regression loss: 0.89519 | Running loss: 1.50887\n",
      "Epoch: 1 | Iteration: 385 | Classification loss: 0.47373 | Regression loss: 0.81214 | Running loss: 1.50809\n",
      "Epoch: 1 | Iteration: 386 | Classification loss: 0.50579 | Regression loss: 0.42077 | Running loss: 1.50657\n",
      "Epoch: 1 | Iteration: 387 | Classification loss: 0.78021 | Regression loss: 0.95462 | Running loss: 1.50652\n",
      "Epoch: 1 | Iteration: 388 | Classification loss: 0.59590 | Regression loss: 0.80735 | Running loss: 1.50573\n",
      "Epoch: 1 | Iteration: 389 | Classification loss: 0.47292 | Regression loss: 0.78330 | Running loss: 1.50631\n",
      "Epoch: 1 | Iteration: 390 | Classification loss: 0.56425 | Regression loss: 1.00257 | Running loss: 1.50589\n",
      "Epoch: 1 | Iteration: 391 | Classification loss: 0.30466 | Regression loss: 0.89748 | Running loss: 1.50500\n",
      "Epoch: 1 | Iteration: 392 | Classification loss: 0.49565 | Regression loss: 0.85318 | Running loss: 1.50452\n",
      "Epoch: 1 | Iteration: 393 | Classification loss: 0.75848 | Regression loss: 0.88114 | Running loss: 1.50411\n",
      "Epoch: 1 | Iteration: 394 | Classification loss: 0.52688 | Regression loss: 0.76010 | Running loss: 1.50362\n",
      "Epoch: 1 | Iteration: 395 | Classification loss: 0.55380 | Regression loss: 1.00557 | Running loss: 1.50323\n",
      "Epoch: 1 | Iteration: 396 | Classification loss: 0.52577 | Regression loss: 0.72990 | Running loss: 1.50242\n",
      "Epoch: 1 | Iteration: 397 | Classification loss: 0.44581 | Regression loss: 0.80715 | Running loss: 1.50178\n",
      "Epoch: 1 | Iteration: 398 | Classification loss: 0.74509 | Regression loss: 0.84698 | Running loss: 1.50154\n",
      "Epoch: 1 | Iteration: 399 | Classification loss: 0.73927 | Regression loss: 0.85232 | Running loss: 1.50118\n",
      "Epoch: 1 | Iteration: 400 | Classification loss: 0.43983 | Regression loss: 0.93464 | Running loss: 1.50132\n",
      "Epoch: 1 | Iteration: 401 | Classification loss: 0.62498 | Regression loss: 0.71864 | Running loss: 1.50324\n",
      "Epoch: 1 | Iteration: 402 | Classification loss: 0.23389 | Regression loss: 0.69862 | Running loss: 1.50184\n",
      "Epoch: 1 | Iteration: 403 | Classification loss: 0.86074 | Regression loss: 1.01108 | Running loss: 1.50265\n",
      "Epoch: 1 | Iteration: 404 | Classification loss: 0.91920 | Regression loss: 0.79293 | Running loss: 1.50253\n",
      "Epoch: 1 | Iteration: 405 | Classification loss: 0.58172 | Regression loss: 0.81543 | Running loss: 1.50174\n",
      "Epoch: 1 | Iteration: 406 | Classification loss: 0.58195 | Regression loss: 0.88343 | Running loss: 1.50113\n",
      "Epoch: 1 | Iteration: 407 | Classification loss: 0.64622 | Regression loss: 0.84532 | Running loss: 1.50084\n",
      "Epoch: 1 | Iteration: 408 | Classification loss: 0.62932 | Regression loss: 0.79813 | Running loss: 1.50081\n",
      "Epoch: 1 | Iteration: 409 | Classification loss: 0.67067 | Regression loss: 0.91882 | Running loss: 1.50063\n",
      "Epoch: 1 | Iteration: 410 | Classification loss: 0.95081 | Regression loss: 0.87482 | Running loss: 1.50084\n",
      "Epoch: 1 | Iteration: 411 | Classification loss: 0.20760 | Regression loss: 0.40856 | Running loss: 1.49846\n",
      "Epoch: 1 | Iteration: 412 | Classification loss: 0.41871 | Regression loss: 0.75081 | Running loss: 1.49811\n",
      "Epoch: 1 | Iteration: 413 | Classification loss: 0.98995 | Regression loss: 0.96992 | Running loss: 1.49862\n",
      "Epoch: 1 | Iteration: 414 | Classification loss: 0.32506 | Regression loss: 0.84559 | Running loss: 1.49484\n",
      "Epoch: 1 | Iteration: 415 | Classification loss: 0.75716 | Regression loss: 0.74475 | Running loss: 1.49455\n",
      "Epoch: 1 | Iteration: 416 | Classification loss: 0.83867 | Regression loss: 0.68781 | Running loss: 1.49580\n",
      "Epoch: 1 | Iteration: 417 | Classification loss: 0.63843 | Regression loss: 0.96529 | Running loss: 1.49540\n",
      "Epoch: 1 | Iteration: 418 | Classification loss: 0.98503 | Regression loss: 0.88845 | Running loss: 1.49580\n",
      "Epoch: 1 | Iteration: 419 | Classification loss: 0.53746 | Regression loss: 0.79162 | Running loss: 1.49637\n",
      "Epoch: 1 | Iteration: 420 | Classification loss: 0.66202 | Regression loss: 0.00000 | Running loss: 1.49415\n",
      "Epoch: 1 | Iteration: 421 | Classification loss: 0.53982 | Regression loss: 0.91665 | Running loss: 1.49371\n",
      "Epoch: 1 | Iteration: 422 | Classification loss: 0.49828 | Regression loss: 0.87287 | Running loss: 1.49282\n",
      "Epoch: 1 | Iteration: 423 | Classification loss: 0.41462 | Regression loss: 0.74508 | Running loss: 1.49314\n",
      "Epoch: 1 | Iteration: 424 | Classification loss: 1.01498 | Regression loss: 0.44053 | Running loss: 1.49214\n",
      "Epoch: 1 | Iteration: 425 | Classification loss: 0.75685 | Regression loss: 0.82444 | Running loss: 1.49154\n",
      "Epoch: 1 | Iteration: 426 | Classification loss: 0.60999 | Regression loss: 0.93350 | Running loss: 1.49108\n",
      "Epoch: 1 | Iteration: 427 | Classification loss: 0.61446 | Regression loss: 0.89276 | Running loss: 1.49065\n",
      "Epoch: 1 | Iteration: 428 | Classification loss: 0.62076 | Regression loss: 0.91647 | Running loss: 1.48997\n",
      "Epoch: 1 | Iteration: 429 | Classification loss: 0.81457 | Regression loss: 1.04370 | Running loss: 1.49056\n",
      "Epoch: 1 | Iteration: 430 | Classification loss: 0.62145 | Regression loss: 0.82433 | Running loss: 1.49126\n",
      "Epoch: 1 | Iteration: 431 | Classification loss: 0.63245 | Regression loss: 0.76962 | Running loss: 1.49053\n",
      "Epoch: 1 | Iteration: 432 | Classification loss: 0.63210 | Regression loss: 0.81477 | Running loss: 1.49008\n",
      "Evaluating dataset\n",
      "168/168\n",
      "mAP:\n",
      "generator.num_classes 2\n",
      "0: 0.08475292155279102\n",
      "Precision:  0.007082971957213067\n",
      "Recall:  0.6901408450704225\n",
      "1: 0.031731197453743004\n",
      "Precision:  0.007082971957213067\n",
      "Recall:  0.6901408450704225\n",
      "Epoch: 2 | Iteration: 0 | Classification loss: 0.06308 | Regression loss: 0.00000 | Running loss: 1.48625\n",
      "Epoch: 2 | Iteration: 1 | Classification loss: 0.56487 | Regression loss: 0.66581 | Running loss: 1.48727\n",
      "Epoch: 2 | Iteration: 2 | Classification loss: 0.73559 | Regression loss: 0.86608 | Running loss: 1.48643\n",
      "Epoch: 2 | Iteration: 3 | Classification loss: 0.78633 | Regression loss: 0.94720 | Running loss: 1.48528\n",
      "Epoch: 2 | Iteration: 4 | Classification loss: 0.38787 | Regression loss: 0.39450 | Running loss: 1.48428\n",
      "Epoch: 2 | Iteration: 5 | Classification loss: 0.64783 | Regression loss: 0.78042 | Running loss: 1.48366\n",
      "Epoch: 2 | Iteration: 6 | Classification loss: 0.63675 | Regression loss: 0.77168 | Running loss: 1.48609\n",
      "Epoch: 2 | Iteration: 7 | Classification loss: 0.59470 | Regression loss: 1.08601 | Running loss: 1.48635\n",
      "Epoch: 2 | Iteration: 8 | Classification loss: 0.47048 | Regression loss: 0.77274 | Running loss: 1.48608\n",
      "Epoch: 2 | Iteration: 9 | Classification loss: 0.30117 | Regression loss: 0.63048 | Running loss: 1.48465\n",
      "Epoch: 2 | Iteration: 10 | Classification loss: 1.21455 | Regression loss: 0.86021 | Running loss: 1.48560\n",
      "Epoch: 2 | Iteration: 11 | Classification loss: 1.04686 | Regression loss: 0.50282 | Running loss: 1.48515\n",
      "Epoch: 2 | Iteration: 12 | Classification loss: 0.63527 | Regression loss: 0.97887 | Running loss: 1.48525\n",
      "Epoch: 2 | Iteration: 13 | Classification loss: 0.54739 | Regression loss: 1.01965 | Running loss: 1.48488\n",
      "Epoch: 2 | Iteration: 14 | Classification loss: 0.57982 | Regression loss: 0.95583 | Running loss: 1.48603\n",
      "Epoch: 2 | Iteration: 15 | Classification loss: 0.58330 | Regression loss: 0.84505 | Running loss: 1.48516\n",
      "Epoch: 2 | Iteration: 16 | Classification loss: 0.54613 | Regression loss: 0.61436 | Running loss: 1.48398\n",
      "Epoch: 2 | Iteration: 17 | Classification loss: 0.42568 | Regression loss: 0.90224 | Running loss: 1.48446\n",
      "Epoch: 2 | Iteration: 18 | Classification loss: 0.44544 | Regression loss: 0.97471 | Running loss: 1.48395\n",
      "Epoch: 2 | Iteration: 19 | Classification loss: 0.70132 | Regression loss: 0.84693 | Running loss: 1.48364\n",
      "Epoch: 2 | Iteration: 20 | Classification loss: 0.65932 | Regression loss: 0.80964 | Running loss: 1.48269\n",
      "Epoch: 2 | Iteration: 21 | Classification loss: 0.36969 | Regression loss: 0.78375 | Running loss: 1.48162\n",
      "Epoch: 2 | Iteration: 22 | Classification loss: 0.61882 | Regression loss: 0.82881 | Running loss: 1.48100\n",
      "Epoch: 2 | Iteration: 23 | Classification loss: 0.53893 | Regression loss: 0.64816 | Running loss: 1.48037\n",
      "Epoch: 2 | Iteration: 24 | Classification loss: 0.65186 | Regression loss: 0.83397 | Running loss: 1.47989\n",
      "Epoch: 2 | Iteration: 25 | Classification loss: 0.52985 | Regression loss: 0.77799 | Running loss: 1.47925\n",
      "Epoch: 2 | Iteration: 26 | Classification loss: 0.52733 | Regression loss: 0.95114 | Running loss: 1.47890\n",
      "Epoch: 2 | Iteration: 27 | Classification loss: 0.61464 | Regression loss: 0.71016 | Running loss: 1.47894\n",
      "Epoch: 2 | Iteration: 28 | Classification loss: 0.46108 | Regression loss: 0.92605 | Running loss: 1.47852\n",
      "Epoch: 2 | Iteration: 29 | Classification loss: 0.52804 | Regression loss: 0.78176 | Running loss: 1.47795\n",
      "Epoch: 2 | Iteration: 30 | Classification loss: 0.60267 | Regression loss: 0.88235 | Running loss: 1.47790\n",
      "Epoch: 2 | Iteration: 31 | Classification loss: 0.55038 | Regression loss: 0.73037 | Running loss: 1.47737\n",
      "Epoch: 2 | Iteration: 32 | Classification loss: 0.49563 | Regression loss: 0.80686 | Running loss: 1.47664\n",
      "Epoch: 2 | Iteration: 33 | Classification loss: 0.44490 | Regression loss: 0.90947 | Running loss: 1.47612\n",
      "Epoch: 2 | Iteration: 34 | Classification loss: 1.28353 | Regression loss: 0.60689 | Running loss: 1.47681\n",
      "Epoch: 2 | Iteration: 35 | Classification loss: 3.04058 | Regression loss: 0.41175 | Running loss: 1.48028\n",
      "Epoch: 2 | Iteration: 36 | Classification loss: 0.61521 | Regression loss: 0.89479 | Running loss: 1.47897\n",
      "Epoch: 2 | Iteration: 37 | Classification loss: 0.47932 | Regression loss: 0.82772 | Running loss: 1.48057\n",
      "Epoch: 2 | Iteration: 38 | Classification loss: 0.50580 | Regression loss: 0.37128 | Running loss: 1.47858\n",
      "Epoch: 2 | Iteration: 39 | Classification loss: 0.50266 | Regression loss: 0.83158 | Running loss: 1.47779\n",
      "Epoch: 2 | Iteration: 40 | Classification loss: 0.22234 | Regression loss: 0.50461 | Running loss: 1.47593\n",
      "Epoch: 2 | Iteration: 41 | Classification loss: 0.48388 | Regression loss: 0.82966 | Running loss: 1.47539\n",
      "Epoch: 2 | Iteration: 42 | Classification loss: 0.27299 | Regression loss: 0.26842 | Running loss: 1.47489\n",
      "Epoch: 2 | Iteration: 43 | Classification loss: 0.50667 | Regression loss: 0.76983 | Running loss: 1.47428\n",
      "Epoch: 2 | Iteration: 44 | Classification loss: 0.66137 | Regression loss: 0.77398 | Running loss: 1.47543\n",
      "Epoch: 2 | Iteration: 45 | Classification loss: 0.34536 | Regression loss: 0.30444 | Running loss: 1.47367\n",
      "Epoch: 2 | Iteration: 46 | Classification loss: 0.40690 | Regression loss: 0.37645 | Running loss: 1.47200\n",
      "Epoch: 2 | Iteration: 47 | Classification loss: 0.67714 | Regression loss: 0.82135 | Running loss: 1.47156\n",
      "Epoch: 2 | Iteration: 48 | Classification loss: 0.66768 | Regression loss: 0.86537 | Running loss: 1.47094\n",
      "Epoch: 2 | Iteration: 49 | Classification loss: 0.84760 | Regression loss: 0.80047 | Running loss: 1.47164\n",
      "Epoch: 2 | Iteration: 50 | Classification loss: 0.72987 | Regression loss: 0.87195 | Running loss: 1.47121\n",
      "Epoch: 2 | Iteration: 51 | Classification loss: 0.27184 | Regression loss: 0.40990 | Running loss: 1.46923\n",
      "Epoch: 2 | Iteration: 52 | Classification loss: 0.54206 | Regression loss: 0.74012 | Running loss: 1.47041\n",
      "Epoch: 2 | Iteration: 53 | Classification loss: 0.59678 | Regression loss: 0.90297 | Running loss: 1.47161\n",
      "Epoch: 2 | Iteration: 54 | Classification loss: 0.59934 | Regression loss: 0.46163 | Running loss: 1.47030\n",
      "Epoch: 2 | Iteration: 55 | Classification loss: 0.67615 | Regression loss: 0.87201 | Running loss: 1.47026\n",
      "Epoch: 2 | Iteration: 56 | Classification loss: 0.37105 | Regression loss: 0.71063 | Running loss: 1.46941\n",
      "Epoch: 2 | Iteration: 57 | Classification loss: 0.03169 | Regression loss: 0.00000 | Running loss: 1.46651\n",
      "Epoch: 2 | Iteration: 58 | Classification loss: 0.80172 | Regression loss: 0.78386 | Running loss: 1.46658\n",
      "Epoch: 2 | Iteration: 59 | Classification loss: 0.45250 | Regression loss: 0.66863 | Running loss: 1.46554\n",
      "Epoch: 2 | Iteration: 60 | Classification loss: 0.59544 | Regression loss: 0.77010 | Running loss: 1.46533\n",
      "Epoch: 2 | Iteration: 61 | Classification loss: 0.45514 | Regression loss: 0.28652 | Running loss: 1.46385\n",
      "Epoch: 2 | Iteration: 62 | Classification loss: 0.46806 | Regression loss: 0.70364 | Running loss: 1.46313\n",
      "Epoch: 2 | Iteration: 63 | Classification loss: 0.63388 | Regression loss: 0.79070 | Running loss: 1.46262\n",
      "Epoch: 2 | Iteration: 64 | Classification loss: 0.96909 | Regression loss: 0.66946 | Running loss: 1.46225\n",
      "Epoch: 2 | Iteration: 65 | Classification loss: 0.47072 | Regression loss: 0.74373 | Running loss: 1.46062\n",
      "Epoch: 2 | Iteration: 66 | Classification loss: 0.66099 | Regression loss: 0.62711 | Running loss: 1.46029\n",
      "Epoch: 2 | Iteration: 67 | Classification loss: 0.45360 | Regression loss: 0.85579 | Running loss: 1.45951\n",
      "Epoch: 2 | Iteration: 68 | Classification loss: 0.58269 | Regression loss: 0.93022 | Running loss: 1.45950\n",
      "Epoch: 2 | Iteration: 69 | Classification loss: 0.30069 | Regression loss: 0.44709 | Running loss: 1.45475\n",
      "Epoch: 2 | Iteration: 70 | Classification loss: 0.30278 | Regression loss: 0.45697 | Running loss: 1.45354\n",
      "Epoch: 2 | Iteration: 71 | Classification loss: 0.68658 | Regression loss: 0.44439 | Running loss: 1.45206\n",
      "Epoch: 2 | Iteration: 72 | Classification loss: 0.41763 | Regression loss: 0.90317 | Running loss: 1.45145\n",
      "Epoch: 2 | Iteration: 73 | Classification loss: 0.47410 | Regression loss: 0.90913 | Running loss: 1.45009\n",
      "Epoch: 2 | Iteration: 74 | Classification loss: 0.59731 | Regression loss: 1.03212 | Running loss: 1.45051\n",
      "Epoch: 2 | Iteration: 75 | Classification loss: 0.34847 | Regression loss: 0.40806 | Running loss: 1.45041\n",
      "Epoch: 2 | Iteration: 76 | Classification loss: 0.61346 | Regression loss: 0.90837 | Running loss: 1.45151\n",
      "Epoch: 2 | Iteration: 77 | Classification loss: 0.42778 | Regression loss: 0.42249 | Running loss: 1.45013\n",
      "Epoch: 2 | Iteration: 78 | Classification loss: 0.58484 | Regression loss: 0.90069 | Running loss: 1.45100\n",
      "Epoch: 2 | Iteration: 79 | Classification loss: 0.43646 | Regression loss: 0.67444 | Running loss: 1.45024\n",
      "Epoch: 2 | Iteration: 80 | Classification loss: 0.37574 | Regression loss: 0.69473 | Running loss: 1.44901\n",
      "Epoch: 2 | Iteration: 81 | Classification loss: 0.47330 | Regression loss: 0.91156 | Running loss: 1.44859\n",
      "Epoch: 2 | Iteration: 82 | Classification loss: 0.63569 | Regression loss: 0.39352 | Running loss: 1.44853\n",
      "Epoch: 2 | Iteration: 83 | Classification loss: 0.87379 | Regression loss: 0.72504 | Running loss: 1.44868\n",
      "Epoch: 2 | Iteration: 84 | Classification loss: 1.08109 | Regression loss: 0.94932 | Running loss: 1.44921\n",
      "Epoch: 2 | Iteration: 85 | Classification loss: 0.87330 | Regression loss: 0.82022 | Running loss: 1.44911\n",
      "Epoch: 2 | Iteration: 86 | Classification loss: 0.81180 | Regression loss: 0.58519 | Running loss: 1.44828\n",
      "Epoch: 2 | Iteration: 87 | Classification loss: 0.23235 | Regression loss: 0.47400 | Running loss: 1.44634\n",
      "Epoch: 2 | Iteration: 88 | Classification loss: 0.32529 | Regression loss: 0.48775 | Running loss: 1.44524\n",
      "Epoch: 2 | Iteration: 89 | Classification loss: 0.53501 | Regression loss: 0.72105 | Running loss: 1.44563\n",
      "Epoch: 2 | Iteration: 90 | Classification loss: 0.63429 | Regression loss: 0.84104 | Running loss: 1.44656\n",
      "Epoch: 2 | Iteration: 91 | Classification loss: 0.55556 | Regression loss: 0.75114 | Running loss: 1.44744\n",
      "Epoch: 2 | Iteration: 92 | Classification loss: 0.39652 | Regression loss: 0.87743 | Running loss: 1.44673\n",
      "Epoch: 2 | Iteration: 93 | Classification loss: 0.54341 | Regression loss: 0.96534 | Running loss: 1.44717\n",
      "Epoch: 2 | Iteration: 94 | Classification loss: 0.72585 | Regression loss: 0.77963 | Running loss: 1.44847\n",
      "Epoch: 2 | Iteration: 95 | Classification loss: 0.51382 | Regression loss: 0.88524 | Running loss: 1.44775\n",
      "Epoch: 2 | Iteration: 96 | Classification loss: 0.46546 | Regression loss: 0.58816 | Running loss: 1.44693\n",
      "Epoch: 2 | Iteration: 97 | Classification loss: 0.64437 | Regression loss: 0.88042 | Running loss: 1.44683\n",
      "Epoch: 2 | Iteration: 98 | Classification loss: 0.57891 | Regression loss: 0.94639 | Running loss: 1.44738\n",
      "Epoch: 2 | Iteration: 99 | Classification loss: 0.56021 | Regression loss: 0.78449 | Running loss: 1.44689\n",
      "Epoch: 2 | Iteration: 100 | Classification loss: 0.46892 | Regression loss: 0.60530 | Running loss: 1.44607\n",
      "Epoch: 2 | Iteration: 101 | Classification loss: 0.55334 | Regression loss: 0.84820 | Running loss: 1.44540\n",
      "Epoch: 2 | Iteration: 102 | Classification loss: 0.50544 | Regression loss: 0.90325 | Running loss: 1.44528\n",
      "Epoch: 2 | Iteration: 103 | Classification loss: 0.80013 | Regression loss: 0.91172 | Running loss: 1.44715\n",
      "Epoch: 2 | Iteration: 104 | Classification loss: 0.94733 | Regression loss: 0.85036 | Running loss: 1.44662\n",
      "Epoch: 2 | Iteration: 105 | Classification loss: 0.72256 | Regression loss: 0.77039 | Running loss: 1.44754\n",
      "Epoch: 2 | Iteration: 106 | Classification loss: 0.65968 | Regression loss: 0.94066 | Running loss: 1.44751\n",
      "Epoch: 2 | Iteration: 107 | Classification loss: 0.68704 | Regression loss: 0.90071 | Running loss: 1.44774\n",
      "Epoch: 2 | Iteration: 108 | Classification loss: 0.58178 | Regression loss: 0.94204 | Running loss: 1.44692\n",
      "Epoch: 2 | Iteration: 109 | Classification loss: 0.79384 | Regression loss: 0.88744 | Running loss: 1.44911\n",
      "Epoch: 2 | Iteration: 110 | Classification loss: 0.78433 | Regression loss: 0.81877 | Running loss: 1.44859\n",
      "Epoch: 2 | Iteration: 111 | Classification loss: 0.79806 | Regression loss: 0.00000 | Running loss: 1.44858\n",
      "Epoch: 2 | Iteration: 112 | Classification loss: 0.30440 | Regression loss: 0.32382 | Running loss: 1.44592\n",
      "Epoch: 2 | Iteration: 113 | Classification loss: 0.57076 | Regression loss: 0.33737 | Running loss: 1.44474\n",
      "Epoch: 2 | Iteration: 114 | Classification loss: 0.63523 | Regression loss: 0.73421 | Running loss: 1.44537\n",
      "Epoch: 2 | Iteration: 115 | Classification loss: 0.75896 | Regression loss: 0.72949 | Running loss: 1.44532\n",
      "Epoch: 2 | Iteration: 116 | Classification loss: 0.51487 | Regression loss: 0.78815 | Running loss: 1.44464\n",
      "Epoch: 2 | Iteration: 117 | Classification loss: 0.54158 | Regression loss: 0.60881 | Running loss: 1.44345\n",
      "Epoch: 2 | Iteration: 118 | Classification loss: 0.31898 | Regression loss: 0.70669 | Running loss: 1.44264\n",
      "Epoch: 2 | Iteration: 119 | Classification loss: 0.20521 | Regression loss: 0.31745 | Running loss: 1.44080\n",
      "Epoch: 2 | Iteration: 120 | Classification loss: 0.83437 | Regression loss: 0.95799 | Running loss: 1.43997\n",
      "Epoch: 2 | Iteration: 121 | Classification loss: 0.33923 | Regression loss: 0.71858 | Running loss: 1.43860\n",
      "Epoch: 2 | Iteration: 122 | Classification loss: 0.48383 | Regression loss: 0.27875 | Running loss: 1.43684\n",
      "Epoch: 2 | Iteration: 123 | Classification loss: 0.26260 | Regression loss: 0.77841 | Running loss: 1.43593\n",
      "Epoch: 2 | Iteration: 124 | Classification loss: 0.69269 | Regression loss: 0.71976 | Running loss: 1.43583\n",
      "Epoch: 2 | Iteration: 125 | Classification loss: 0.41534 | Regression loss: 0.77824 | Running loss: 1.43451\n",
      "Epoch: 2 | Iteration: 126 | Classification loss: 0.27735 | Regression loss: 0.00000 | Running loss: 1.43176\n",
      "Epoch: 2 | Iteration: 127 | Classification loss: 0.37622 | Regression loss: 0.73440 | Running loss: 1.43149\n",
      "Epoch: 2 | Iteration: 128 | Classification loss: 0.31761 | Regression loss: 0.95139 | Running loss: 1.43100\n",
      "Epoch: 2 | Iteration: 129 | Classification loss: 0.70957 | Regression loss: 1.04526 | Running loss: 1.43208\n",
      "Epoch: 2 | Iteration: 130 | Classification loss: 0.67689 | Regression loss: 0.73966 | Running loss: 1.43203\n",
      "Epoch: 2 | Iteration: 131 | Classification loss: 0.15769 | Regression loss: 0.34025 | Running loss: 1.43039\n",
      "Epoch: 2 | Iteration: 132 | Classification loss: 0.64854 | Regression loss: 0.64871 | Running loss: 1.42964\n",
      "Epoch: 2 | Iteration: 133 | Classification loss: 0.49835 | Regression loss: 0.43555 | Running loss: 1.42923\n",
      "Epoch: 2 | Iteration: 134 | Classification loss: 0.05031 | Regression loss: 0.00000 | Running loss: 1.42631\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "# from retinanet import model\n",
    "# from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "#     Normalizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from retinanet import coco_eval\n",
    "# from retinanet import csv_eval\n",
    "\n",
    "assert torch.__version__.split('.')[0] == '1'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "\n",
    "def main(train_file, class_list, test_file=None,depth=50,epochs=100):\n",
    "    print('Simple training script for training a RetinaNet network.')\n",
    "    # Create the data loaders\n",
    "    dataset_train = CSVDataset(train_file, class_list,\n",
    "                               transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "\n",
    "    dataset_val = CSVDataset(test_file, class_list,\n",
    "                                     transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "    sampler = AspectRatioBasedSampler(dataset_train, batch_size=2, drop_last=False)\n",
    "    dataloader_train = DataLoader(dataset_train, collate_fn=collater, batch_sampler=sampler)#\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "        dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
    "#     print(dataset_train.num_classes())\n",
    "    # Create the model\n",
    "    if depth == 18:\n",
    "        retinanet = resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif depth == 34:\n",
    "        retinanet = resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif depth == 50:\n",
    "        retinanet = resnet50(num_classes=2, pretrained=True)\n",
    "    elif depth == 101:\n",
    "        retinanet = resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif depth == 152:\n",
    "        retinanet = resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')\n",
    "\n",
    "    use_gpu = True\n",
    "\n",
    "    if use_gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            retinanet = retinanet.cuda()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "    else:\n",
    "        retinanet = torch.nn.DataParallel(retinanet)\n",
    "\n",
    "    retinanet.training = True\n",
    "\n",
    "    optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "    loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "    retinanet.train()\n",
    "    \n",
    "    retinanet.module.freeze_bn()\n",
    "\n",
    "    print('Num training images: {}'.format(len(dataset_train)))\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        retinanet.train()\n",
    "        retinanet.module.freeze_bn()\n",
    "        epoch_loss = []\n",
    "        for iter_num, data in enumerate(dataloader_train):\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "                else:\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].float(), data['annot']])\n",
    "                    \n",
    "                classification_loss = classification_loss.mean()\n",
    "                regression_loss = regression_loss.mean()\n",
    "\n",
    "                loss = classification_loss + regression_loss\n",
    "\n",
    "                if bool(loss == 0):\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_hist.append(float(loss))\n",
    "\n",
    "                epoch_loss.append(float(loss))\n",
    "#                 plt.plot(epoch_num,epoch_loss)\n",
    "#             # naming the x axis \n",
    "#                 plt.xlabel('Recall') \n",
    "#             # naming the y axis \n",
    "#                 plt.ylabel('Precision') \n",
    "\n",
    "#             # giving a title to my graph \n",
    "#                 plt.title('Precision Recall curve') \n",
    "\n",
    "#             # function to show the plot\n",
    "#                 plt.show()\n",
    "                print(\n",
    "                    'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
    "                        epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "                del classification_loss\n",
    "                del regression_loss\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "        if  test_file is not None:\n",
    "\n",
    "            print('Evaluating dataset')\n",
    "\n",
    "            mAP = evaluate(dataset_val, retinanet)\n",
    "\n",
    "        scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "#         torch.save('{}_retinanet.pt'.format(epoch_num))\n",
    "\n",
    "    retinanet.eval()\n",
    "\n",
    "    torch.save(retinanet, 'model_final.pt')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(r'/hy-tmp/deeplesion/gan.csv',\n",
    "         r'/hy-tmp/deeplesion/map.csv',\n",
    "         r'/hy-tmp/deeplesion/gan1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4159b7-0bd8-4e1f-a06b-a4b848ae5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import argparse\n",
    "\n",
    "\n",
    "def load_classes(csv_reader):\n",
    "    result = {}\n",
    "    for line, row in enumerate(csv_reader):\n",
    "        line += 1\n",
    "\n",
    "        try:\n",
    "            class_name, class_id = row\n",
    "        except ValueError:\n",
    "            raise(ValueError('line {}: format should be \\'class_name,class_id\\''.format(line)))\n",
    "        class_id = int(class_id)\n",
    "\n",
    "        if class_name in result:\n",
    "            raise ValueError('line {}: duplicate class name: \\'{}\\''.format(line, class_name))\n",
    "        result[class_name] = class_id\n",
    "    return result\n",
    "\n",
    "\n",
    "# Draws a caption above the box in an image\n",
    "def draw_caption(image, box, caption):\n",
    "    b = np.array(box).astype(int)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "def detect_image(image_path, model_path, class_list):\n",
    "\n",
    "    with open(class_list, 'r') as f:\n",
    "        classes = load_classes(csv.reader(f, delimiter=','))\n",
    "\n",
    "    labels = {}\n",
    "    for key, value in classes.items():\n",
    "        labels[value] = key\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.training = False\n",
    "    model.eval()\n",
    "\n",
    "    for img_name in os.listdir(image_path):\n",
    "\n",
    "        image = cv2.imread(os.path.join(image_path, img_name))\n",
    "        if image is None:\n",
    "            continue\n",
    "        image_orig = image.copy()\n",
    "\n",
    "        rows, cols, cns = image.shape\n",
    "\n",
    "        smallest_side = min(rows, cols)\n",
    "\n",
    "        # rescale the image so the smallest side is min_side\n",
    "        min_side = 608\n",
    "        max_side = 1024\n",
    "        scale = min_side / smallest_side\n",
    "\n",
    "        # check if the largest side is now greater than max_side, which can happen\n",
    "        # when images have a large aspect ratio\n",
    "        largest_side = max(rows, cols)\n",
    "\n",
    "        if largest_side * scale > max_side:\n",
    "            scale = max_side / largest_side\n",
    "\n",
    "        # resize the image with the computed scale\n",
    "        image = cv2.resize(image, (int(round(cols * scale)), int(round((rows * scale)))))\n",
    "        rows, cols, cns = image.shape\n",
    "\n",
    "        pad_w = 32 - rows % 32\n",
    "        pad_h = 32 - cols % 32\n",
    "\n",
    "        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n",
    "        new_image[:rows, :cols, :] = image.astype(np.float32)\n",
    "        image = new_image.astype(np.float32)\n",
    "        image /= 255\n",
    "        image -= [0.485, 0.456, 0.406]\n",
    "        image /= [0.229, 0.224, 0.225]\n",
    "        image = np.expand_dims(image, 0)\n",
    "        image = np.transpose(image, (0, 3, 1, 2))\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            image = torch.from_numpy(image)\n",
    "            if torch.cuda.is_available():\n",
    "                image = image.cuda()\n",
    "\n",
    "            st = time.time()\n",
    "            print(image.shape, image_orig.shape, scale)\n",
    "            scores, classification, transformed_anchors = model(image.cuda().float())\n",
    "            print('Elapsed time: {}'.format(time.time() - st))\n",
    "            idxs = np.where(scores.cpu() > 0.5)\n",
    "\n",
    "            for j in range(idxs[0].shape[0]):\n",
    "                bbox = transformed_anchors[idxs[0][j], :]\n",
    "\n",
    "                x1 = int(bbox[0] / scale)\n",
    "                y1 = int(bbox[1] / scale)\n",
    "                x2 = int(bbox[2] / scale)\n",
    "                y2 = int(bbox[3] / scale)\n",
    "                label_name = labels[int(classification[idxs[0][j]])]\n",
    "                print(bbox, classification.shape)\n",
    "                score = scores[j]\n",
    "                caption = '{} {:.3f}'.format(label_name, score)\n",
    "                # draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "                draw_caption(image_orig, (x1, y1, x2, y2), caption)\n",
    "                cv2.rectangle(image_orig, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n",
    "\n",
    "            cv2.imshow('detections', image_orig)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Simple script for visualizing result of training.')\n",
    "\n",
    "#     parser.add_argument('--image_dir', help='Path to directory containing images')\n",
    "#     parser.add_argument('--model_path', help='Path to model')\n",
    "#     parser.add_argument('--class_list', help='Path to CSV file listing class names (see README)')\n",
    "\n",
    "print('Simple script for visualizing result of training')\n",
    "image_dir=r'/hy-tmp/deeplesion/train'\n",
    "model_path=r'model_final.pt'\n",
    "class_list=r'/hy-tmp/deeplesion/map.csv'\n",
    "detect_image(image_dir, model_path, class_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b1e0498-9a25-4be7-9c60-b9c6b6fd2565",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8014f3-bba8-4977-9ab6-5e162f4cd547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
